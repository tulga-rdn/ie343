{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 5 (Full mark: 100pt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use ``Google Colab`` if you would like to use GPU.\n",
    "- https://colab.research.google.com/notebooks/welcome.ipynb\n",
    "- https://theorydb.github.io/dev/2019/08/23/dev-ml-colab/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Regression (50pt)\n",
    "\n",
    "**For this question, using PyTorch, implement the 1) ridge regression and 2) Lasso. You can refer to the tutorial link below for how to implement linear regression. Note that the ridge regression is the linear regression with L2 penalty, and the Lasso is the linear regression with L1 penalty. You should use ````Boston```` dataset as shown in the code below. You should not only write the code for the models, but also train them and show the test MSE.**\n",
    "- https://github.com/yunjey/pytorch-tutorial/blob/master/tutorials/01-basics/linear_regression/main.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "<img src=\"figures/regressions.png\" width=\"700\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "# To fix the random seed\n",
    "torch.manual_seed(0)\n",
    "torch.cuda.manual_seed_all(0)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# load data\n",
    "boston = pd.read_csv('data/Boston.csv').drop('Unnamed: 0', axis=1)\n",
    "data = torch.FloatTensor(boston.values)\n",
    "X = data[:,:-1] # Input (X)\n",
    "y = data[:,-1].reshape(-1, 1) # Ground Truth (y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/100], Loss: 408.6328\n",
      "Epoch [10/100], Loss: 205.7512\n",
      "Epoch [15/100], Loss: 141.6315\n",
      "Epoch [20/100], Loss: 120.5482\n",
      "Epoch [25/100], Loss: 113.3427\n",
      "Epoch [30/100], Loss: 110.6189\n",
      "Epoch [35/100], Loss: 109.3489\n",
      "Epoch [40/100], Loss: 108.5565\n",
      "Epoch [45/100], Loss: 107.9269\n",
      "Epoch [50/100], Loss: 107.3585\n",
      "Epoch [55/100], Loss: 106.8181\n",
      "Epoch [60/100], Loss: 106.2949\n",
      "Epoch [65/100], Loss: 105.7851\n",
      "Epoch [70/100], Loss: 105.2873\n",
      "Epoch [75/100], Loss: 104.8007\n",
      "Epoch [80/100], Loss: 104.3250\n",
      "Epoch [85/100], Loss: 103.8597\n",
      "Epoch [90/100], Loss: 103.4045\n",
      "Epoch [95/100], Loss: 102.9593\n",
      "Epoch [100/100], Loss: 102.5235\n",
      "Test loss:  87.6945571899414\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/loss.py:529: UserWarning: Using a target size (torch.Size([404, 1])) that is different to the input size (torch.Size([404, 13])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/loss.py:529: UserWarning: Using a target size (torch.Size([102, 1])) that is different to the input size (torch.Size([102, 13])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 100\n",
    "learning_rate = 0.00003\n",
    "l2_lambda = 1\n",
    "\n",
    "class Ridge(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super().__init__()\n",
    "        ## Write your answer here\n",
    "        self.model = nn.Linear(input_dim, input_dim)\n",
    "    def forward(self, X):\n",
    "        ## Write your answer here\n",
    "        return self.model(X)\n",
    "\n",
    "# Linear regression model\n",
    "model = Ridge(X_train.shape[1])\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.MSELoss() \n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)  \n",
    "\n",
    "# Train the model\n",
    "for epoch in range(num_epochs):\n",
    "    l2_norm = sum(p.pow(2.0).sum() for p in model.parameters())\n",
    "    inputs = X_train\n",
    "    targets = y_train\n",
    "\n",
    "    # Forward pass\n",
    "    outputs = model(inputs)\n",
    "    loss = criterion(outputs, targets) + l2_lambda*l2_norm\n",
    "    \n",
    "    # Backward and optimize\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if (epoch+1) % 5 == 0:\n",
    "        print ('Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, num_epochs, loss.item()))\n",
    "        \n",
    "print(\"Test loss: \", float(criterion(model(X_test), y_test)))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/100], Loss: 950.6422\n",
      "Epoch [10/100], Loss: 305.8843\n",
      "Epoch [15/100], Loss: 169.9936\n",
      "Epoch [20/100], Loss: 139.9421\n",
      "Epoch [25/100], Loss: 132.0015\n",
      "Epoch [30/100], Loss: 128.7459\n",
      "Epoch [35/100], Loss: 126.5395\n",
      "Epoch [40/100], Loss: 124.6196\n",
      "Epoch [45/100], Loss: 122.8235\n",
      "Epoch [50/100], Loss: 121.1141\n",
      "Epoch [55/100], Loss: 119.4806\n",
      "Epoch [60/100], Loss: 117.9181\n",
      "Epoch [65/100], Loss: 116.4228\n",
      "Epoch [70/100], Loss: 114.9914\n",
      "Epoch [75/100], Loss: 113.6210\n",
      "Epoch [80/100], Loss: 112.3086\n",
      "Epoch [85/100], Loss: 111.0515\n",
      "Epoch [90/100], Loss: 109.8470\n",
      "Epoch [95/100], Loss: 108.6927\n",
      "Epoch [100/100], Loss: 107.5862\n",
      "Test loss:  76.2603530883789\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 100\n",
    "learning_rate = 0.000003\n",
    "l1_lambda = 1\n",
    "        \n",
    "class Lasso(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super().__init__()\n",
    "        ## Write your answer here\n",
    "        self.model = nn.Linear(input_dim, 1)\n",
    "        \n",
    "    def forward(self, X):\n",
    "        ## Write your answer here\n",
    "        return self.model(X)\n",
    "\n",
    "model = Lasso(X_train.shape[1])\n",
    "# Loss and optimizer\n",
    "criterion = nn.MSELoss() \n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)  \n",
    "\n",
    "# Train the model\n",
    "for epoch in range(num_epochs):\n",
    "    l1_norm = sum(p.abs().sum() for p in model.parameters())\n",
    "    inputs = X_train\n",
    "    targets = y_train\n",
    "\n",
    "    # Forward pass\n",
    "    outputs = model(inputs)\n",
    "    loss = criterion(outputs, targets) + l1_lambda*l1_norm\n",
    "    \n",
    "    # Backward and optimize\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if (epoch+1) % 5 == 0:\n",
    "        print ('Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, num_epochs, loss.item()))\n",
    "        \n",
    "print(\"Test loss: \", float(criterion(model(X_test), y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Autoencoder (50pt)\n",
    "**Autoencoder is an unsupervised neural network model for learning representations of the input. In the figure below, you can see the structure of an autoencoder network. Given the original input image, we first encode the image using ``Encoder`` to a compressed representation, and reconstruct the image by using ``Decoder`` given the compressed representation. The compressed represesntation can be used as the dimension reduced representation of the original input. In this regard, autoencoder is also known as a model for dimensionality reduction (Recall PCA was also a method for dimensionality reduction). Note that the encoder and the decoder can be any neural network model such as MLP, CNN, MLP, etc.**\n",
    "\n",
    "**For this question, you will use MNIST dataset to implement two versions of autoencoders: 1) An MLP-based autoencoder, and 2) A CNN-based autoencoder. The figure below shows an example of the MLP-based autoencoder. For the MLP-based autoencoder, you should follow the structure of the autoencoder shown in the figure below. For the CNN-based autoencoder, you are free to choose the architecture. You only need to implement ``Autoencoder_MLP`` class and ``Autoencoder_CNN`` class. Note that for ``Autoencoder_MLP``, you should flatten the original image into a vector, whereas for ``Autoencoder_CNN``, you can use the original image without any modification. After implementing these two classes, save the figures and observe the results. Write a few sentences to describe your findings. You do not need to submit the saved figure and the dataset for your final submission.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "<img src=\"figures/autoencoder.png\" width=\"700\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch [1/100], loss:0.1694\n",
      "epoch [2/100], loss:0.1441\n",
      "epoch [3/100], loss:0.1350\n",
      "epoch [4/100], loss:0.1269\n",
      "epoch [5/100], loss:0.1250\n",
      "epoch [6/100], loss:0.1211\n",
      "epoch [7/100], loss:0.1239\n",
      "epoch [8/100], loss:0.1241\n",
      "epoch [9/100], loss:0.1203\n",
      "epoch [10/100], loss:0.1139\n",
      "epoch [11/100], loss:0.1216\n",
      "epoch [12/100], loss:0.1027\n",
      "epoch [13/100], loss:0.1156\n",
      "epoch [14/100], loss:0.1162\n",
      "epoch [15/100], loss:0.1086\n",
      "epoch [16/100], loss:0.1079\n",
      "epoch [17/100], loss:0.1083\n",
      "epoch [18/100], loss:0.1053\n",
      "epoch [19/100], loss:0.1129\n",
      "epoch [20/100], loss:0.1071\n",
      "epoch [21/100], loss:0.1071\n",
      "epoch [22/100], loss:0.1098\n",
      "epoch [23/100], loss:0.1007\n",
      "epoch [24/100], loss:0.1052\n",
      "epoch [25/100], loss:0.1005\n",
      "epoch [26/100], loss:0.1018\n",
      "epoch [27/100], loss:0.1064\n",
      "epoch [28/100], loss:0.1040\n",
      "epoch [29/100], loss:0.1027\n",
      "epoch [30/100], loss:0.1069\n",
      "epoch [31/100], loss:0.1048\n",
      "epoch [32/100], loss:0.1021\n",
      "epoch [33/100], loss:0.0966\n",
      "epoch [34/100], loss:0.1066\n",
      "epoch [35/100], loss:0.1001\n",
      "epoch [36/100], loss:0.0999\n",
      "epoch [37/100], loss:0.0969\n",
      "epoch [38/100], loss:0.1034\n",
      "epoch [39/100], loss:0.1112\n",
      "epoch [40/100], loss:0.1035\n",
      "epoch [41/100], loss:0.0977\n",
      "epoch [42/100], loss:0.1044\n",
      "epoch [43/100], loss:0.1146\n",
      "epoch [44/100], loss:0.1018\n",
      "epoch [45/100], loss:0.0941\n",
      "epoch [46/100], loss:0.1058\n",
      "epoch [47/100], loss:0.1063\n",
      "epoch [48/100], loss:0.0984\n",
      "epoch [49/100], loss:0.1046\n",
      "epoch [50/100], loss:0.0949\n",
      "epoch [51/100], loss:0.1018\n",
      "epoch [52/100], loss:0.0975\n",
      "epoch [53/100], loss:0.0929\n",
      "epoch [54/100], loss:0.1037\n",
      "epoch [55/100], loss:0.1058\n",
      "epoch [56/100], loss:0.1067\n",
      "epoch [57/100], loss:0.1041\n",
      "epoch [58/100], loss:0.1060\n",
      "epoch [59/100], loss:0.0965\n",
      "epoch [60/100], loss:0.1017\n",
      "epoch [61/100], loss:0.0977\n",
      "epoch [62/100], loss:0.1074\n",
      "epoch [63/100], loss:0.1011\n",
      "epoch [64/100], loss:0.1059\n",
      "epoch [65/100], loss:0.0985\n",
      "epoch [66/100], loss:0.1030\n",
      "epoch [67/100], loss:0.0904\n",
      "epoch [68/100], loss:0.0965\n",
      "epoch [69/100], loss:0.0944\n",
      "epoch [70/100], loss:0.0982\n",
      "epoch [71/100], loss:0.1036\n",
      "epoch [72/100], loss:0.0963\n",
      "epoch [73/100], loss:0.0994\n",
      "epoch [74/100], loss:0.0970\n",
      "epoch [75/100], loss:0.0965\n",
      "epoch [76/100], loss:0.0952\n",
      "epoch [77/100], loss:0.1054\n",
      "epoch [78/100], loss:0.0990\n",
      "epoch [79/100], loss:0.1006\n",
      "epoch [80/100], loss:0.0999\n",
      "epoch [81/100], loss:0.0929\n",
      "epoch [82/100], loss:0.0979\n",
      "epoch [83/100], loss:0.1074\n",
      "epoch [84/100], loss:0.0983\n",
      "epoch [85/100], loss:0.0921\n",
      "epoch [86/100], loss:0.0967\n",
      "epoch [87/100], loss:0.1032\n",
      "epoch [88/100], loss:0.0879\n",
      "epoch [89/100], loss:0.0927\n",
      "epoch [90/100], loss:0.1038\n",
      "epoch [91/100], loss:0.1022\n",
      "epoch [92/100], loss:0.0976\n",
      "epoch [93/100], loss:0.1000\n",
      "epoch [94/100], loss:0.0967\n",
      "epoch [95/100], loss:0.1047\n",
      "epoch [96/100], loss:0.0902\n",
      "epoch [97/100], loss:0.0922\n",
      "epoch [98/100], loss:0.1009\n",
      "epoch [99/100], loss:0.0974\n",
      "epoch [100/100], loss:0.1007\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "# Make a directory \"saved_img\" if it does not exist\n",
    "if not os.path.exists('./saved_img'):\n",
    "    os.mkdir('./saved_img')\n",
    "\n",
    "\n",
    "def to_img(x):\n",
    "    x = 0.5 * (x + 1)\n",
    "    x = x.clamp(0, 1)\n",
    "    x = x.view(x.size(0), 1, 28, 28)\n",
    "    return x\n",
    "\n",
    "\n",
    "num_epochs = 100\n",
    "batch_size = 128\n",
    "learning_rate = 1e-3\n",
    "\n",
    "img_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5], [0.5])\n",
    "])\n",
    "\n",
    "dataset = MNIST('./data', transform=img_transform)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "\n",
    "class Autoencoder_MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Autoencoder_MLP, self).__init__()\n",
    "        ## Write your answer here\n",
    "        self.encoder = nn.Sequential(nn.Linear(784,128),\n",
    "                                     nn.ReLU(),\n",
    "                                     nn.Linear(128,64),\n",
    "                                     nn.ReLU(),\n",
    "                                     nn.Linear(64,12),\n",
    "                                     nn.ReLU(),\n",
    "                                     nn.Linear(12,3))\n",
    "        self.decoder = nn.Sequential(nn.Linear(3,12),\n",
    "                                     nn.ReLU(),\n",
    "                                     nn.Linear(12,64),\n",
    "                                     nn.ReLU(),\n",
    "                                     nn.Linear(64,128),\n",
    "                                     nn.ReLU(),\n",
    "                                     nn.Linear(128,784))\n",
    "\n",
    "    def forward(self, x):\n",
    "        ## Write your answer here\n",
    "        latent = self.encoder(x)\n",
    "        out = self.decoder(latent)\n",
    "        return out\n",
    "\n",
    "class Autoencoder_CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Autoencoder_CNN, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(1, 8, 3, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(8, 16, 3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(16, 32, 3, stride=2, padding=0),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(start_dim=1),\n",
    "            nn.Linear(3 * 3 * 32, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 4)\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(4, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 3 * 3 * 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Unflatten(dim=1, unflattened_size=(32, 3, 3)),\n",
    "            nn.ConvTranspose2d(32, 16, 3, stride=2, output_padding=0),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(16, 8, 3, stride=2, \n",
    "            padding=1, output_padding=1),\n",
    "            nn.BatchNorm2d(8),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(8, 1, 3, stride=2, padding=1, output_padding=1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        latent = self.encoder(x)\n",
    "        out = self.decoder(latent)\n",
    "        return out\n",
    "\n",
    "# Uncomment below correspondingly\n",
    "\n",
    "#model = Autoencoder_MLP()\n",
    "model = Autoencoder_CNN()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-5)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for data in dataloader:\n",
    "        img, _ = data\n",
    "        \n",
    "        # Flatten the image for Autoencoder_MLP (You can remove this line for Autoencoder_CNN)\n",
    "        #img = img.view(img.size(0), -1)\n",
    "        \n",
    "        # forward pass\n",
    "        output = model(img)\n",
    "        loss = criterion(output, img)\n",
    "        # backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Print log and save images\n",
    "    print('epoch [{}/{}], loss:{:.4f}'.format(epoch + 1, num_epochs, loss.item()))\n",
    "    if epoch % 10 == 0:\n",
    "        pic = to_img(output.cpu().data)\n",
    "        save_image(pic, './saved_img/cnn_image_{}.png'.format(epoch))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table><tr>\n",
    "<td> <img src=\"figures/cnn_image_90.png\" alt=\"Drawing\" style=\"width: 250px;\"/> </td>\n",
    "<td> <img src=\"figures/mlp_image_50.png\" alt=\"Drawing\" style=\"width: 250px;\"/> </td>\n",
    "</tr></table>\n",
    "Left: CNN best epoch output, right: MLP best epoch output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the CNN output is much clearer than MLP output. In CNN output, the digits are very clear, with practically no noise, whereas in MLP output image, the digits are \"washed out\" and there are sometimes \"ghost\" of another digit along with the actual digit. We can probably improve the MLP output by changing the architecture (changing the dimensions of the layers or the activation functions) or by hyperparameter (learning rate) tuning, but by design, CNN will most likely perform as it, by definition, can learn the spatial structure between the nearby pixels, which improves its denoising capabilities and the images are clearer and more accurate than those denoised by MLP"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
