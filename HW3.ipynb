{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 3 (Full mark: 250pt)\n",
    "- Questions 1~5: Conceptual\n",
    "- Questions 6~11: Programming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conceptual questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Variance (10pt)\n",
    "\n",
    "**Using basic statistical properties of the variance, as well as single variable calculus, derive the below equation. In other words, prove that $\\alpha$ given below does indeed minimize $\\operatorname{Var}(\\alpha X+(1-\\alpha) Y)$.**\n",
    "\n",
    "$$\n",
    "\\begin{array}{c}\n",
    "\\alpha=\\frac{\\sigma_{Y}^{2}-\\sigma_{X Y}}{\\sigma_{X}^{2}+\\sigma_{Y}^{2}-2 \\sigma_{X Y}}\n",
    "\\end{array}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To minimize the $\\operatorname{Var}(\\alpha X+(1-\\alpha) Y)$, we should find the critical points (at which derivative of the expression is equal to 0). The expression can be rearranged to $\\alpha^{2}\\sigma^{2}_{X}+(1-\\alpha)^{2}\\sigma^{2}_{Y}+2\\alpha(1-\\alpha)\\sigma_{XY}$, and its derivative is \n",
    "$$\n",
    "\\begin{array}\n",
    "=\\frac{\\partial}{\\partial \\alpha}\\alpha^{2}\\sigma^{2}_{X}+(1-\\alpha)^{2}\\sigma^{2}_{Y}+2\\alpha(\\alpha-1)\\sigma_{XY}=2\\alpha\\sigma^{2}_{X}-2(1-\\alpha)\\sigma^{2}_{Y}+2(1-2\\alpha)\\sigma_{XY}\n",
    "\\end{array}\n",
    "$$\n",
    "The minimum is the point where the derivative is 0, so we can then do some basic algebraic transformations:\n",
    "$$\n",
    "\\begin{array}\n",
    "=2\\alpha\\sigma^{2}_{X}-2\\sigma^{2}_{Y}+2\\alpha\\sigma^{2}_{Y}+2\\sigma_{XY}-4\\alpha\\sigma_{XY}=0\n",
    "\\end{array}\n",
    "$$\n",
    "$$\n",
    "\\begin{array}\n",
    "=2\\alpha(\\sigma^{2}_{X}+\\sigma^{2}_{Y}-2\\sigma_{XY})=2\\sigma^{2}_{Y}-2\\sigma_{XY}\n",
    "\\end{array}\n",
    "$$\n",
    "$$\n",
    "\\begin{array}{c}\n",
    "\\alpha=\\frac{\\sigma_{Y}^{2}-\\sigma_{X Y}}{\\sigma_{X}^{2}+\\sigma_{Y}^{2}-2 \\sigma_{X Y}}\n",
    "\\end{array}\n",
    "$$\n",
    "As the initial expression is a quadratic function, i.e. a parabola, the minimum is the only one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Estimating the standard deviation of prediction (10pt)\n",
    "**Suppose that we use some statistical learning method to make a prediction for the response Y for a particular value of the predictor X. Carefully describe how we might estimate the standard deviation of our prediction.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In practice, it is almost impossible to generate new samples to estimate the standard deviation of our prediction, so we need to use the data from the original dataset. To estimate the standard deviation of our prediction using the bootstrap method, we randomly select $n$ observations (dubbed the bootstrap dataset $Z^{*r}$) with replacement $B$ times. A bootstrap dataset $Z^{*r}$ can be used to produce a bootstrap estimate for output value $\\hat{\\alpha}^{*r}$. The standard error of these estimate is\n",
    "$$\n",
    "\\begin{array}{c}\n",
    "SE_{B}(\\hat{\\alpha})=\\sqrt{\\frac{1}{B-1}\\sum \\limits _{r=1} ^{B} (\\hat{\\alpha}^{*r}-\\frac{1}{B} \\sum \\limits _{r'=1} ^{B} \\hat{\\alpha}^{*r'})^2}\n",
    "\\end{array}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Subset selection method (15pt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We perform best subset, forward stepwise, and backward stepwise selection on a single data set. For each approach, we obtain $p + 1$ models, containing $0, 1, 2,..., p$ predictors. Explain your answers:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(a) Which of the three models with $k$ predictors has the smallest training RSS?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model with best subset selection has the smallest training RSS since it considers every possible combination with k predictors for model selection. On the other hand, both forward and backward stepwise selections consider a limited number of models as they are heuristic algorithms, meaning that bad initialization values will result in sub-optimal results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(b) Which of the three models with $k$ predictors has the smallest test RSS?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we cannot determine the performance of the models on the test dataset, it is impossible to say which model will have the small test RSS. The model with best subset selection might have the smallest test RSS, but the stepwise selection models can have smaller test RSS as there is uncertainty and chance involved. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(c) True or False:\n",
    "- i. The predictors in the k-variable model identified by forward stepwise are a subset of the predictors in the (k+1)-variable model identified by forward stepwise selection.\n",
    "- ii. The predictors in the k-variable model identified by backward stepwise are a subset of the predictors in the (k + 1)- variable model identified by backward stepwise selection.\n",
    "- iii. The predictors in the k-variable model identified by backward stepwise are a subset of the predictors in the (k + 1)- variable model identified by forward stepwise selection.\n",
    "- iv. The predictors in the k-variable model identified by forward stepwise are a subset of the predictors in the (k+1)-variable model identified by backward stepwise selection.\n",
    "- v. The predictors in the k-variable model identified by best subset are a subset of the predictors in the (k + 1)-variable model identified by best subset selection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "i. True\n",
    "ii. True\n",
    "iii. False\n",
    "iv. False\n",
    "v. False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Lasso (25pt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we estimate the regression coefficients in a linear regression model by minimizing\n",
    "$$\n",
    "\\sum_{i=1}^{n}\\left(y_{i}-\\beta_{0}-\\sum_{j=1}^{p} \\beta_{j} x_{i j}\\right)^{2} \\quad \\text { subject to } \\quad \\sum_{j=1}^{p}\\left|\\beta_{j}\\right| \\leq s\n",
    "$$\n",
    "for a particular value of $s$. For parts (a) through (e), indicate which of i. through v. is correct. Justify your answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(a) As we increase $s$ from 0 , the training RSS will:**\n",
    "- i. Increase initially, and then eventually start decreasing in an inverted U shape.\n",
    "- ii. Decrease initially, and then eventually start increasing in a U shape.\n",
    "- iii. Steadily increase.\n",
    "- iv. Steadily decrease.\n",
    "- v . Remain constant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "iv. Steadily decrease as increase in $s$ results in higher flexibility, which leads to lower RSS. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(b) Repeat (a) for test RSS.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ii. Decrease initially, and then eventually start increasing in a U shape. The initial decrease in RSS is a result of faster decrease in bias due to higher flexibility as a result of lower $s$, but after some breakpoint, there will be a faster increase in variance, resulting in a increase in RSS due to lower flexibility as a result of higher $s$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(c) Repeat (a) for variance.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "iii. Steadily increase, as more flexibility (from the increase in $s$) leads to more variance. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(d) Repeat (a) for (squared) bias.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "iv. Steadily decrease, as more flexibility (from the increase in $s$) leads to less bias. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(e) Repeat (a) for the irreducible error.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "v. Remain constant, as the irreducible error is always present regardless of the model and of $s$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Ridge regression (25pt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we estimate the regression coefficients in a linear regression model by minimizing\n",
    "$$\n",
    "\\sum_{i=1}^{n}\\left(y_{i}-\\beta_{0}-\\sum_{j=1}^{p} \\beta_{j} x_{i j}\\right)^{2}+\\lambda \\sum_{j=1}^{p} \\beta_{j}^{2}\n",
    "$$\n",
    "for a particular value of $\\lambda$. For parts (a) through (e), indicate which of i. through $\\mathrm{v}$. is correct. Justify your answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(a) As we increase $\\lambda$ from 0 , the training RSS will:**\n",
    "- i. Increase initially, and then eventually start decreasing in an inverted U shape.\n",
    "- ii. Decrease initially, and then eventually start increasing in a U shape.\n",
    "- iii. Steadily increase.\n",
    "- iv. Steadily decrease.\n",
    "- v. Remain constant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "iii. Steadily increase as increase in $\\lambda$ results in lower flexibility, which leads to higher RSS."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(b) Repeat (a) for test RSS.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ii. Decrease initially, and then eventually start increasing in a U shape. The initial decrease in RSS is a result of faster decrease in variance due to higher flexibility as a result of lower $\\lambda$, but after some breakpoint, there will be a faster increase in bias, resulting in a increase in RSS due to lower flexibility as a result of higher $\\lambda$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(c) Repeat (a) for variance.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "iv. Steadily decrease, as less flexibility (from the increase in $\\lambda$) leads to less variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(d) Repeat (a) for (squared) bias.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "iii. Steadily decrease, as less flexibility (from the increase in $\\lambda$) leads to more bias."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(e) Repeat (a) for the irreducible error.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "v. Remain constant, as the irreducible error is always present regardless of the model and of $\\lambda$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Programming questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Validation Set Approach (20pt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Chapter 4, we used logistic regression to predict the probability of ````default```` using ````income```` and ````balance```` on the ````Default```` data set. We will now estimate the test error of this logistic regression model using the validation set approach. Do not forget to set a random seed before beginning your analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>default</th>\n",
       "      <th>student</th>\n",
       "      <th>balance</th>\n",
       "      <th>income</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>729.526495</td>\n",
       "      <td>44361.62507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>817.180407</td>\n",
       "      <td>12106.13470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>1073.549164</td>\n",
       "      <td>31767.13895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>529.250605</td>\n",
       "      <td>35704.49394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>785.655883</td>\n",
       "      <td>38463.49588</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  default student      balance       income\n",
       "1      No      No   729.526495  44361.62507\n",
       "2      No     Yes   817.180407  12106.13470\n",
       "3      No      No  1073.549164  31767.13895\n",
       "4      No      No   529.250605  35704.49394\n",
       "5      No      No   785.655883  38463.49588"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, LeaveOneOut, KFold\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import warnings\n",
    "\n",
    "# print numpy arrays with precision 4\n",
    "np.set_printoptions(precision=4)\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "df = pd.read_csv('data/Default.csv',index_col=0)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(a) Fit a logistic regression model that uses ````income```` and ````balance```` to predict ````default````.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(random_state=0)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X, y = df[[\"income\", \"balance\"]], df[\"default\"]\n",
    "logreg_model = LogisticRegression(random_state=0)\n",
    "logreg_model.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(b) Using the validation set approach, estimate the test error of this model. In order to do this, you must perform the following steps:**\n",
    "- i. Split the sample set into a training set and a validation set (75%:25%).\n",
    "- ii. Fit a multiple logistic regression model using only the training observations.\n",
    "- iii. Obtain a prediction of default status for each individual in the validation set by computing the posterior probability of default for that individual, and classifying the individual to the ````default```` category if the posterior probability is greater than 0.5.\n",
    "- iv. Compute the validation set error, which is the fraction of the observations in the validation set that are misclassified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.038799999999999946\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=0)\n",
    "logreg_model_b = LogisticRegression(random_state=0)\n",
    "logreg_model_b.fit(X_train, y_train)\n",
    "print(1-logreg_model_b.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(c) Repeat the process in (b) three times, using three different splits of the observations into a training set and a validation set. Comment on the results obtained.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 split validation set error: 0.03159999999999996\n",
      "1 split validation set error: 0.027200000000000002\n",
      "2 split validation set error: 0.03080000000000005\n"
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=i+1)\n",
    "    logreg_model_c = LogisticRegression(random_state=0)\n",
    "    logreg_model_c.fit(X_train, y_train)\n",
    "    print(f\"{i} split validation set error: {1-logreg_model_b.score(X_test, y_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results are fairly consistent, with errors around 0.03, and the errors are within about 10-20% from each other. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(d) Now consider a logistic regression model that predicts the probability of ````default```` using ````income````, ````balance````, and a dummy variable for ````student````. Estimate the test error for this model using the validation set approach. Comment on whether or not including a dummy variable for ````student```` leads to a reduction in the test error rate.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.03759999999999997\n"
     ]
    }
   ],
   "source": [
    "X_n, y = df[[\"income\", \"balance\"]], df[\"default\"]\n",
    "d = pd.get_dummies(df[[\"student\"]],drop_first=True)\n",
    "X = pd.concat([X_n, d], axis = 1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=10)\n",
    "\n",
    "logreg_model_d = LogisticRegression(random_state=10)\n",
    "logreg_model_d.fit(X_train, y_train)\n",
    "print(1-logreg_model_d.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Including a dummy variable does not lead to a reduction in test error rate. Moreover, it is higher than the average error in part (c)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Leave-one-out cross validation (LOOCV) (25pt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Compute the LOOCV error for a simple logistic regression model on the Weekly data set.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "weekly = pd.read_csv(\"data/Weekly.csv\")\n",
    "X = weekly[['Lag1', 'Lag2']]\n",
    "y = weekly['Direction']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(a) Fit a logistic regression model that predicts ````Direction```` using ````Lag1```` and ````Lag2````."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(random_state=13)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logreg_model_a = LogisticRegression(random_state=13)\n",
    "logreg_model_a.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b) Fit a logistic regression model that predicts ````Direction```` using ````Lag1```` and ````Lag2```` using all but the first observation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(random_state=123)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_b = X.loc[X.index > 0]\n",
    "y_b = y.loc[y.index > 0]\n",
    "\n",
    "logreg_model_b = LogisticRegression(random_state=123)\n",
    "logreg_model_b.fit(X_b, y_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(c) Use the model from (b) to predict the direction of the first observation. You can do this by predicting that the first observation will go up if $P(Direction=\"Up\"|Lag1, Lag2) > 0.5$. Was this observation correctly classified?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted probabilities: [[0.4286 0.5714]]\n",
      "Actual value: 0    Down\n",
      "Name: Direction, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(f\"Predicted probabilities: {logreg_model_b.predict_proba(X.loc[X.index == 0])}\")\n",
    "print(f\"Actual value: {y.loc[y.index == 0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the posterior probability predicted by the model is 0.4286 that the direction will be down and 0.5714 that the direction will be up, the model's resulting prediction will be \"up\". Thus, the observation is not correctly classified as the actual value is \"down\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(d) Write a for loop from $i = 1$ to $i = n$, where $n$ is the number of observations in the data set, that performs each of the following steps:\n",
    "- i. Fit a logistic regression model using all but the ith observation to predict ````Direction```` using ````Lag1```` and ````Lag2````.\n",
    "- ii. Compute the posterior probability of the market moving up for the ith observation.\n",
    "- iii. Use the posterior probability for the ith observation in order to predict whether or not the market moves up.\n",
    "- iv. Determine whether or not an error was made in predicting the direction for the ith observation. If an error was made, then indicate this as a 1, and otherwise indicate it as a 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = []\n",
    "for i in range(len(X)):\n",
    "    X_i = X.loc[X.index != i]\n",
    "    y_i = y.loc[y.index != i]\n",
    "    logreg_model_i = LogisticRegression()\n",
    "    logreg_model_i.fit(X_i, y_i)\n",
    "    scores.append(1-int(logreg_model_i.score(X.loc[X.index == i], y.loc[y.index == i])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(e) Take the average of the $n$ numbers obtained in (d)iv in order to obtain the LOOCV estimate for the test error. Comment on the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.44995408631772266"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(scores)/len(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The LOOCV estimate is 0.45, meaning that the logistic regression method (with probability threshold of 0.5) performs almost the same as random guessing. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Shrinkage methods + Dimension reduction (35pt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise, we will predict the number of applications received using the other variables in the ````College```` data set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(a) Split the data set into a training set and a test set.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "college = pd.read_csv(\"data/College.csv\")\n",
    "X_n = college[college.columns.difference(['Unnamed: 0', 'Apps', 'Private'])]\n",
    "y = college[\"Apps\"]\n",
    "d = pd.get_dummies(college[[\"Private\"]],drop_first=True)\n",
    "X = pd.concat([X_n, d], axis = 1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Private</th>\n",
       "      <th>Apps</th>\n",
       "      <th>Accept</th>\n",
       "      <th>Enroll</th>\n",
       "      <th>Top10perc</th>\n",
       "      <th>Top25perc</th>\n",
       "      <th>F.Undergrad</th>\n",
       "      <th>P.Undergrad</th>\n",
       "      <th>Outstate</th>\n",
       "      <th>Room.Board</th>\n",
       "      <th>Books</th>\n",
       "      <th>Personal</th>\n",
       "      <th>PhD</th>\n",
       "      <th>Terminal</th>\n",
       "      <th>S.F.Ratio</th>\n",
       "      <th>perc.alumni</th>\n",
       "      <th>Expend</th>\n",
       "      <th>Grad.Rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Abilene Christian University</td>\n",
       "      <td>Yes</td>\n",
       "      <td>1660</td>\n",
       "      <td>1232</td>\n",
       "      <td>721</td>\n",
       "      <td>23</td>\n",
       "      <td>52</td>\n",
       "      <td>2885</td>\n",
       "      <td>537</td>\n",
       "      <td>7440</td>\n",
       "      <td>3300</td>\n",
       "      <td>450</td>\n",
       "      <td>2200</td>\n",
       "      <td>70</td>\n",
       "      <td>78</td>\n",
       "      <td>18.1</td>\n",
       "      <td>12</td>\n",
       "      <td>7041</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Adelphi University</td>\n",
       "      <td>Yes</td>\n",
       "      <td>2186</td>\n",
       "      <td>1924</td>\n",
       "      <td>512</td>\n",
       "      <td>16</td>\n",
       "      <td>29</td>\n",
       "      <td>2683</td>\n",
       "      <td>1227</td>\n",
       "      <td>12280</td>\n",
       "      <td>6450</td>\n",
       "      <td>750</td>\n",
       "      <td>1500</td>\n",
       "      <td>29</td>\n",
       "      <td>30</td>\n",
       "      <td>12.2</td>\n",
       "      <td>16</td>\n",
       "      <td>10527</td>\n",
       "      <td>56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Adrian College</td>\n",
       "      <td>Yes</td>\n",
       "      <td>1428</td>\n",
       "      <td>1097</td>\n",
       "      <td>336</td>\n",
       "      <td>22</td>\n",
       "      <td>50</td>\n",
       "      <td>1036</td>\n",
       "      <td>99</td>\n",
       "      <td>11250</td>\n",
       "      <td>3750</td>\n",
       "      <td>400</td>\n",
       "      <td>1165</td>\n",
       "      <td>53</td>\n",
       "      <td>66</td>\n",
       "      <td>12.9</td>\n",
       "      <td>30</td>\n",
       "      <td>8735</td>\n",
       "      <td>54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Agnes Scott College</td>\n",
       "      <td>Yes</td>\n",
       "      <td>417</td>\n",
       "      <td>349</td>\n",
       "      <td>137</td>\n",
       "      <td>60</td>\n",
       "      <td>89</td>\n",
       "      <td>510</td>\n",
       "      <td>63</td>\n",
       "      <td>12960</td>\n",
       "      <td>5450</td>\n",
       "      <td>450</td>\n",
       "      <td>875</td>\n",
       "      <td>92</td>\n",
       "      <td>97</td>\n",
       "      <td>7.7</td>\n",
       "      <td>37</td>\n",
       "      <td>19016</td>\n",
       "      <td>59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Alaska Pacific University</td>\n",
       "      <td>Yes</td>\n",
       "      <td>193</td>\n",
       "      <td>146</td>\n",
       "      <td>55</td>\n",
       "      <td>16</td>\n",
       "      <td>44</td>\n",
       "      <td>249</td>\n",
       "      <td>869</td>\n",
       "      <td>7560</td>\n",
       "      <td>4120</td>\n",
       "      <td>800</td>\n",
       "      <td>1500</td>\n",
       "      <td>76</td>\n",
       "      <td>72</td>\n",
       "      <td>11.9</td>\n",
       "      <td>2</td>\n",
       "      <td>10922</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>772</th>\n",
       "      <td>Worcester State College</td>\n",
       "      <td>No</td>\n",
       "      <td>2197</td>\n",
       "      <td>1515</td>\n",
       "      <td>543</td>\n",
       "      <td>4</td>\n",
       "      <td>26</td>\n",
       "      <td>3089</td>\n",
       "      <td>2029</td>\n",
       "      <td>6797</td>\n",
       "      <td>3900</td>\n",
       "      <td>500</td>\n",
       "      <td>1200</td>\n",
       "      <td>60</td>\n",
       "      <td>60</td>\n",
       "      <td>21.0</td>\n",
       "      <td>14</td>\n",
       "      <td>4469</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>773</th>\n",
       "      <td>Xavier University</td>\n",
       "      <td>Yes</td>\n",
       "      <td>1959</td>\n",
       "      <td>1805</td>\n",
       "      <td>695</td>\n",
       "      <td>24</td>\n",
       "      <td>47</td>\n",
       "      <td>2849</td>\n",
       "      <td>1107</td>\n",
       "      <td>11520</td>\n",
       "      <td>4960</td>\n",
       "      <td>600</td>\n",
       "      <td>1250</td>\n",
       "      <td>73</td>\n",
       "      <td>75</td>\n",
       "      <td>13.3</td>\n",
       "      <td>31</td>\n",
       "      <td>9189</td>\n",
       "      <td>83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>774</th>\n",
       "      <td>Xavier University of Louisiana</td>\n",
       "      <td>Yes</td>\n",
       "      <td>2097</td>\n",
       "      <td>1915</td>\n",
       "      <td>695</td>\n",
       "      <td>34</td>\n",
       "      <td>61</td>\n",
       "      <td>2793</td>\n",
       "      <td>166</td>\n",
       "      <td>6900</td>\n",
       "      <td>4200</td>\n",
       "      <td>617</td>\n",
       "      <td>781</td>\n",
       "      <td>67</td>\n",
       "      <td>75</td>\n",
       "      <td>14.4</td>\n",
       "      <td>20</td>\n",
       "      <td>8323</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>775</th>\n",
       "      <td>Yale University</td>\n",
       "      <td>Yes</td>\n",
       "      <td>10705</td>\n",
       "      <td>2453</td>\n",
       "      <td>1317</td>\n",
       "      <td>95</td>\n",
       "      <td>99</td>\n",
       "      <td>5217</td>\n",
       "      <td>83</td>\n",
       "      <td>19840</td>\n",
       "      <td>6510</td>\n",
       "      <td>630</td>\n",
       "      <td>2115</td>\n",
       "      <td>96</td>\n",
       "      <td>96</td>\n",
       "      <td>5.8</td>\n",
       "      <td>49</td>\n",
       "      <td>40386</td>\n",
       "      <td>99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>776</th>\n",
       "      <td>York College of Pennsylvania</td>\n",
       "      <td>Yes</td>\n",
       "      <td>2989</td>\n",
       "      <td>1855</td>\n",
       "      <td>691</td>\n",
       "      <td>28</td>\n",
       "      <td>63</td>\n",
       "      <td>2988</td>\n",
       "      <td>1726</td>\n",
       "      <td>4990</td>\n",
       "      <td>3560</td>\n",
       "      <td>500</td>\n",
       "      <td>1250</td>\n",
       "      <td>75</td>\n",
       "      <td>75</td>\n",
       "      <td>18.1</td>\n",
       "      <td>28</td>\n",
       "      <td>4509</td>\n",
       "      <td>99</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>777 rows Ã— 19 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                         Unnamed: 0 Private   Apps  Accept  Enroll  Top10perc  \\\n",
       "0      Abilene Christian University     Yes   1660    1232     721         23   \n",
       "1                Adelphi University     Yes   2186    1924     512         16   \n",
       "2                    Adrian College     Yes   1428    1097     336         22   \n",
       "3               Agnes Scott College     Yes    417     349     137         60   \n",
       "4         Alaska Pacific University     Yes    193     146      55         16   \n",
       "..                              ...     ...    ...     ...     ...        ...   \n",
       "772         Worcester State College      No   2197    1515     543          4   \n",
       "773               Xavier University     Yes   1959    1805     695         24   \n",
       "774  Xavier University of Louisiana     Yes   2097    1915     695         34   \n",
       "775                 Yale University     Yes  10705    2453    1317         95   \n",
       "776    York College of Pennsylvania     Yes   2989    1855     691         28   \n",
       "\n",
       "     Top25perc  F.Undergrad  P.Undergrad  Outstate  Room.Board  Books  \\\n",
       "0           52         2885          537      7440        3300    450   \n",
       "1           29         2683         1227     12280        6450    750   \n",
       "2           50         1036           99     11250        3750    400   \n",
       "3           89          510           63     12960        5450    450   \n",
       "4           44          249          869      7560        4120    800   \n",
       "..         ...          ...          ...       ...         ...    ...   \n",
       "772         26         3089         2029      6797        3900    500   \n",
       "773         47         2849         1107     11520        4960    600   \n",
       "774         61         2793          166      6900        4200    617   \n",
       "775         99         5217           83     19840        6510    630   \n",
       "776         63         2988         1726      4990        3560    500   \n",
       "\n",
       "     Personal  PhD  Terminal  S.F.Ratio  perc.alumni  Expend  Grad.Rate  \n",
       "0        2200   70        78       18.1           12    7041         60  \n",
       "1        1500   29        30       12.2           16   10527         56  \n",
       "2        1165   53        66       12.9           30    8735         54  \n",
       "3         875   92        97        7.7           37   19016         59  \n",
       "4        1500   76        72       11.9            2   10922         15  \n",
       "..        ...  ...       ...        ...          ...     ...        ...  \n",
       "772      1200   60        60       21.0           14    4469         40  \n",
       "773      1250   73        75       13.3           31    9189         83  \n",
       "774       781   67        75       14.4           20    8323         49  \n",
       "775      2115   96        96        5.8           49   40386         99  \n",
       "776      1250   75        75       18.1           28    4509         99  \n",
       "\n",
       "[777 rows x 19 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "college"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(b) Fit a linear model using least squares on the training set, and report the test error obtained.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test error = 838.1485731990109\n",
      "r2 = 0.9519895833504566\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "lr_model = LinearRegression()\n",
    "lr_model.fit(X_train, y_train)\n",
    "print(f\"test error = {np.sqrt(mean_squared_error(lr_model.predict(X_test), y_test))}\")\n",
    "print(f\"r2 = {r2_score(lr_model.predict(X_test), y_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(c) Fit a ridge regression model on the training set, with $\\lambda$ chosen by cross-validation. Report the test error obtained.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test error = 875.3601532397632\n",
      "r2 = 0.9458220446284191\n",
      "lambda = 0.1\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import RidgeCV\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "kf = KFold(n_splits=10)\n",
    "ridge = RidgeCV(normalize=True, cv=kf)\n",
    "ridge.fit(X_train, y_train)\n",
    "print(f\"test error = {np.sqrt(mean_squared_error(ridge.predict(X_test), y_test))}\")\n",
    "print(f\"r2 = {r2_score(ridge.predict(X_test), y_test)}\")\n",
    "print(f\"lambda = {ridge.alpha_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(d) Fit a lasso model on the training set, with $\\lambda$ chosen by crossvalidation. Report the test error obtained, along with the number of non-zero coefficient estimates.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test error = 824.756413503479\n",
      "r2 = 0.9539411706421733\n",
      "coefficients = [1.5062633040297284, 0.016253592977252408, -0.28366058006849015, 0.09083408112352356, 8.860912783473099, -0.0838086415344641, 0.019348361414545184, 0.01503221335163475, -9.30348570203294, 0.1480198327716888, 13.463397021142768, -2.7596167043235584, 39.57478029643669, -6.294857956352189, -0.4714902180067615, -414.16175866946963]\n",
      "lambda = 0.6410905249526215\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Lasso, LassoCV\n",
    "lasso = LassoCV(normalize=True, cv=kf)\n",
    "lasso.fit(X_train, y_train)\n",
    "\n",
    "print(f\"test error = {np.sqrt(mean_squared_error(lasso.predict(X_test), y_test))}\")\n",
    "print(f\"r2 = {r2_score(lasso.predict(X_test), y_test)}\")\n",
    "print(f\"coefficients = {[x for x in lasso.coef_ if x !=0]}\")\n",
    "print(f\"lambda = {lasso.alpha_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(e) Fit a PCR model on the training set, with $M$ chosen by crossvalidation. Report the test error obtained, along with the value of $M$ selected by cross-validation.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M = 17\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.preprocessing import scale \n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA()\n",
    "X_pca = pca.fit_transform(scale(X_train))\n",
    "\n",
    "# 10-fold CV, with shuffle\n",
    "n = len(X_pca)\n",
    "\n",
    "lin_reg2 = LinearRegression()\n",
    "mse2 = []\n",
    "r2 = []\n",
    "\n",
    "# MSE with only the intercept\n",
    "score = -1 * cross_val_score(lin_reg2, np.ones((n,1)), y_train, cv=kf, scoring='neg_mean_squared_error').mean()\n",
    "mse2.append(np.sqrt(score))\n",
    "\n",
    "# MSE for the 19 principle components\n",
    "for i in np.arange(1, 40):\n",
    "    score = -1 * cross_val_score(lin_reg2, X_pca[:,:i], y_train, cv=kf, scoring='neg_mean_squared_error').mean()\n",
    "    mse2.append(np.sqrt(score))\n",
    "\n",
    "min_mse = np.array(mse2).argmin()\n",
    "    \n",
    "print(f\"M = {min_mse}\")\n",
    "plt.plot(mse2, '-D')\n",
    "plt.xlabel('# of PC')\n",
    "plt.ylabel('MSE')\n",
    "plt.xticks(range(40), range(40))\n",
    "min_mse_marker, = plt.plot(min_mse, min(mse2), 'b*', markersize=15)\n",
    "plt.legend([min_mse_marker], ['Best number of principal components']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test error = 2117.506391670812\n",
      "r2 = 0.6623981462480428\n"
     ]
    }
   ],
   "source": [
    "X_train_pca3 = pca.fit_transform(scale(X_train))[:, :7]\n",
    "X_test_pca3 = pca.fit_transform(scale(X_test))[:, :7]\n",
    "\n",
    "lin_reg3 = LinearRegression()\n",
    "lin_reg3.fit(X_train_pca3, y_train)\n",
    "\n",
    "print(f\"test error = {np.sqrt(mean_squared_error(lin_reg3.predict(X_test_pca3), y_test))}\")\n",
    "print(f\"r2 = {r2_score(lin_reg3.predict(X_test_pca3), y_test)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(f) Fit a PLS model on the training set, with $M$ chosen by crossvalidation. Report the test error obtained, along with the value of $M$ selected by cross-validation.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M = 13\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAEGCAYAAACUzrmNAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAAA0/UlEQVR4nO3deXhUVbb4/e+qSkgCMiiDQMIkAgJJCBAQQaFxANTbArYoXAcQlFbp/mn3T2y57VXbfnzbFvtq83arrwqiNhcB57YdURAVFAMyC5g0CAGEMMkUQob1/nFOQpFUpVKVqlSG9XmeenJqn9r7rKpU1apz9j77iKpijDHGVMYT6wCMMcbUfpYsjDHGBGXJwhhjTFCWLIwxxgRlycIYY0xQcbEOIFpatWqlnTt3jnUYxhhTp6xatWq/qrYuX15vk0Xnzp3JysqKdRjGGFOniMgP/sqjdhhKRDqIyBIR+U5ENorI3W75OSLysYh87/4926fODBHJFpEtIjLSp7y/iKx3180SEYlW3MYYYyqKZp9FEfB/VbUnMAiYJiK9gPuBT1S1G/CJex933XigNzAKeFpEvG5bzwBTgW7ubVQU4zbGGFNO1JKFqu5R1dXu8lHgOyAZGA285D7sJWCMuzwaeFVVC1R1G5ANDBSRdkAzVV2hzunmL/vUMcYYUwNqpM9CRDoDfYGvgXNVdQ84CUVE2rgPSwa+8qmW65YVusvly/1tZyrOHggdO3aM4DMw0VJYWEhubi4nT56MdSjGNCiJiYmkpKQQHx9fpcdHPVmIyFnA68A9qnqkku4Gfyu0kvKKharPAc8BZGZmhjXp1fKc/UxftI6Z49IZ3LVVOE2YEOTm5tK0aVM6d+6MdUUZUzNUlQMHDpCbm0uXLl2qVCeq51mISDxOopinqm+4xXvdQ0u4f/e55blAB5/qKcButzzFT3nELc/Zz5S5Wew6nM+UuVksz9kfjc0YHydPnqRly5YhJYoDB2DUKOevMSZ0IkLLli1D2qOP5mgoAWYD36nq//isegeY6C5PBN72KR8vIgki0gWnI3ule8jqqIgMctu8xadOxJQmivzCYgDyC4stYdSQUPco5s6FDz+El14K+lBjTAChfu6iuWcxBLgZuFRE1ri3q4DHgCtE5HvgCvc+qroRWAhsAj4ApqlqsdvWncALOJ3eOcD7kQy0fKIoZQmj9lGFJ590lp980rlvjIm+aI6G+kJVRVXTVTXDvb2nqgdU9TJV7eb+PehT51FV7aqqPVT1fZ/yLFVNddf9SiN8EY7pi9ZVSBSl8guLmb5oXSQ3Z6rh88/hp5+c5cOH4Ysvqt+m1+slIyODPn360K9fP5YvXx5WO0899RQnTpyofkAhePjhh3niiSeivp3PP/+c3r17k5GRQX5+flhtPPjggyxevDisuldddRWHDx8Oq+6kSZN47bXXwqpblyxdujTs925V2NxQwMxx6STFe/2uS4r3MnNceg1HZAJ56ik4ftxZPn789F5GdSQlJbFmzRrWrl3Ln/70J2bMmBFmbDWfLKpDVSkpKanSY+fNm8e9997LmjVrSEpKCnlbxcXFPPLII1x++eUh1wV47733aNGiRVh1GwpLFjVgcNdWzJ6UWSFhJMV7mT0p00ZFxcjo0SBy5u1f/zp96EnVuV/+MaNHh7/NI0eOcPbZZZMKMHPmTAYMGEB6ejoPPfQQAMePH+fqq6+mT58+pKamsmDBAmbNmsXu3bsZPnw4w4cPr9Bu586deeihh+jXrx9paWls3rwZqLhnkJqayvbt29m+fTsXXHABt912G6mpqdx4440sXryYIUOG0K1bN1auXFlWZ+3atVx66aV069aN559/vtLYt2/fTs+ePbnrrrvo168fO3fuPCPOTz75hL59+5KWlsbkyZMpKCjghRdeYOHChTzyyCPceOONZzy+NM6JEyeSnp7OddddV5YwO3fuzCOPPMLFF1/MokWLzviFH+j1OHbsGLfeeitpaWmkp6fz+uuvlz1+//79lW7vkUceYcCAAaSmpjJ16lSCHYDIzs7m8ssvL9ujzMnJQVWZPn06qamppKWlsWDBAsD5Ih42bBjXX3893bt35/7772fevHkMHDiQtLQ0cnJyAGcv5o477uCSSy6he/fuvPvuu4AzkKP0efXt25clS5YAMHfuXK699lpGjRpFt27duO+++8ri++ijj7jooovo168f48aN49ixYwFfu+3bt/Pss8/y5JNPkpGRweeff86iRYtITU2lT58+DB06tNLXokpUtV7e+vfvr6H6MjtPu//+Pe30u3e123/9S7/Mzgu5DROaTZs2BVy3YYNqx46qiYmqTmqo/JaYqNqpk1MvFB6PR/v06aM9evTQZs2aaVZWlqqqfvjhh3r77bdrSUmJFhcX69VXX62fffaZvvbaa3rbbbeV1T98+LCqqnbq1Enz8vy/Zzp16qSzZs1SVdW///3vOmXKFFVVfeihh3TmzJllj+vdu7du27ZNt23bpl6vV9etW6fFxcXar18/vfXWW7WkpETfeustHT16dFn99PR0PXHihObl5WlKSoru2rUrYOzbtm1TEdEVK1ZUiDE/P19TUlJ0y5Ytqqp6880365NPPqmqqhMnTtRFixZVqLNt2zYF9IsvvlBV1VtvvbXs+XTq1En//Oc/lz3Wt41Ar8d9992nd999d1mdgwcPnvHaVra9AwcOlNW76aab9J133qk09oEDB+obb7xR9tyPHz+ur732ml5++eVaVFSkP/74o3bo0EF3796tS5Ys0ebNm+vu3bv15MmT2r59e33wwQdVVfWpp54qi3nixIk6cuRILS4u1q1bt2pycrLm5+frE088oZMmTVJV1e+++047dOig+fn5+uKLL2qXLl308OHDmp+frx07dtQdO3ZoXl6eXnLJJXrs2DFVVX3sscf0D3/4Q6WvXfn3Umpqqubm5qqq6qFDhyo8f1X/nz8gS/18p9qehY/BXVvx3C39ARjZu53tUcRY796waRNccw00blz5Yxs3dvYoNm506oWi9DDU5s2b+eCDD7jllltQVT766CM++ugj+vbtS79+/di8eTPff/89aWlpLF68mN/97nd8/vnnNG/evErbufbaawHo378/27dvD/r4Ll26kJaWhsfjoXfv3lx22WWICGlpaWfUHz16NElJSbRq1Yrhw4ezcuXKgLEDdOrUiUGDBlXY3pYtW+jSpQvdu3cHYOLEiSxbtixonB06dGDIkCEA3HTTTXzh05F0ww03hPR6LF68mGnTppU9xncvL9j2lixZwoUXXkhaWhqffvopGzduDLjto0ePsmvXLsaOHQs4J6g1btyYL774ggkTJuD1ejn33HMZNmwY33zzDQADBgygXbt2JCQk0LVrV0aMGAFQ4f9x/fXX4/F46NatG+eddx6bN2/miy++4OabbwbgggsuoFOnTmzduhWAyy67jObNm5OYmEivXr344Ycf+Oqrr9i0aRNDhgwhIyODl156iR9+OD2/X1XeS0OGDGHSpEk8//zzFBf775MNRb2ddTZcw7q3oXf7Zhw6cSrWoRigSRNYsACefRbuuQcKCio+JiEB/vIXuOOO6m/voosuYv/+/eTl5aGqzJgxg1/+8pcVHrdq1Sree+89ZsyYwYgRI3jwwQeDtp2QkAA4HepFRUUAxMXFndFv4DvuvfTxAB6Pp+y+x+Mpqw8Vh0CKSMDYt2/fTpMmTfzGp2GOG/G3/VKBtgX+Xw9VDTqk09/2Tp48yV133UVWVhYdOnTg4YcfrvQcgkDPtbLXoLr/j6q0W/paqCpXXHEF8+fPr7SO72tX3rPPPsvXX3/Nv/71LzIyMlizZg0tW7YMGEcwtmfhR3pKc9bv+insD4+JvH79nKTgT0IC9O8fme1s3ryZ4uJiWrZsyciRI5kzZ07ZseJdu3axb98+du/eTePGjbnpppu49957Wb16NQBNmzbl6NGjIW2vc+fOZfVXr17Ntm3bQo757bff5uTJkxw4cIClS5cyYMCAgLFX5oILLmD79u1kZ2cD8MorrzBs2LCg29+xYwcrVqwAYP78+Vx88cUhP4dSI0aM4G9/+1vZ/UOHDlVpe6WJoVWrVhw7dizo6KdmzZqRkpLCW2+9BUBBQQEnTpxg6NChLFiwgOLiYvLy8li2bBkDBw4M6TksWrSIkpIScnJy+Pe//02PHj0YOnQo8+bNA2Dr1q3s2LGDHj16BGxj0KBBfPnll2X/ixMnTpTtiQRS/v2Xk5PDhRdeyCOPPEKrVq0q9E+FypKFH6nJzfkpv5DcQ+ENETSRl5UFhYXOsohz2Kn0B1xhobM+XPn5+WRkZJCRkcENN9zASy+9hNfrZcSIEfznf/4nF110EWlpaVx33XUcPXqU9evXM3DgQDIyMnj00Ud54IEHAJg6dSpXXnml3w7uQH7xi19w8OBBMjIyeOaZZ8oOAYVi4MCBXH311QwaNIj//u//pn379gFjr0xiYiIvvvgi48aNKzv8dUcVdtd69uzJSy+9RHp6OgcPHuTOO+8M+TmUeuCBBzh06FBZx2xpR3Cw7bVo0YLbb7+dtLQ0xowZw4ABA4Ju65VXXmHWrFmkp6czePBgfvzxR8aOHUt6ejp9+vTh0ksv5fHHH6dt27YhPYcePXowbNgwrrzySp599lkSExO56667KC4uJi0tjRtuuIG5c+eesUdRXuvWrZk7dy4TJkwgPT2dQYMGlQ0CCOTnP/85b775ZlkH9/Tp00lLSyM1NZWhQ4fSp0+fkJ5HBf46MurDLZwO7lJrdx7STr97V99duzvsNkzVVNbB7Wv8+DM7sd9888zO7wkTohqmCWDbtm3au3fveru9UAXqTK+trIO7mnq0bUq8V1i/66dYh2JcX38NXu/pTuwxY053fnu9znpjTPRYB7cfCXFeup/blA2WLGqNnj3hgQdg8uTTZaWd33PmgDsc39Swzp07s2HDhnq7vVDNnTs31iFEjSWLANKSm/P+hh+rNDrDVE9VXuN//SvwusmTz0wixpjgNMQBPHYYKgDr5K4ZiYmJHDhwwEaeGVODVJ3rWSQmJla5ju1ZBJCW7JxotX7XT3Q4J8gZYSZsKSkp5ObmkpeXF+tQjGlQSq+UV1WWLALo0bYpcR6nk/uqtHaxDqfeio+Pr/KVuowxsWOHoQJIjLdObmOMKWXJohJpyXYmtzHGgCWLSqWmNOfwCevkNsYYSxaVKO3ktkNRxpiGLmrJQkTmiMg+EdngU9ZHRFaIyHoR+aeINPNZN0NEskVki4iM9Cnv7z4+W0RmSQ2e9HCBTye3McY0ZNHcs5gLjCpX9gJwv6qmAW8C0wFEpBcwHujt1nlaREovW/cMMBXo5t7Ktxk1pZ3cliyMMQ1d1JKFqi4DDpYr7gGUXk3lY+AX7vJo4FVVLVDVbUA2MFBE2gHNVHWFO8HVy8CYaMXsT1pyczZYJ7cxpoGr6T6LDcA17vI4oIO7nAz4Trae65Ylu8vly2tMakpzDlkntzGmgavpZDEZmCYiq4CmQOnl6Pz1Q2gl5X6JyFQRyRKRrEidEWyd3MYYU8PJQlU3q+oIVe0PzAdy3FW5nN7LAEgBdrvlKX7KA7X/nKpmqmpm69atIxKzdXIbY0wNJwsRaeP+9QAPAM+6q94BxotIgoh0wenIXqmqe4CjIjLIHQV1C/B2TcacGO+lm3VyG2MauGgOnZ0PrAB6iEiuiEwBJojIVmAzzh7CiwCquhFYCGwCPgCmqWqx29SdOKOosnH2RN6PVsyBpCU3s05uY0yDFrWJBFV1QoBVfw3w+EeBR/2UZwGpEQwtZGnJzVmYlcuuw/mknG0z0BpjGh47g7sKUq2T2xjTwFmyqIKe7ZrhtU5uY0wDZsmiChLjvXRrcxbrdx2JdSjGGBMTliyqyM7kNsY0ZJYsqigtpTkHj59i908nYx2KMcbUOEsWVVR2Te5c67cwxjQ8liyqqLST20ZEGWMaIksWVXS6k9uShTGm4bFkEQLr5DbGNFSWLEKQltKcA9bJbYxpgCxZhCDVOrmNMQ2UJYsQ9LJObmNMA2XJIgTWyW2MaagsWYQo1Tq5jTENkCWLEKUlO53ce6yT2xjTgFiyCFFZJ7cdijLGNCCWLELUq10zPGLXtjDGNCyWLEKU1MhLtzZ2TW5jTMNiySIMaSnWyW2MaViilixEZI6I7BORDT5lGSLylYisEZEsERnos26GiGSLyBYRGelT3l9E1rvrZomIRCvmqkpLbs7+Y6f48Yh1chtjGoZo7lnMBUaVK3sc+IOqZgAPuvcRkV7AeKC3W+dpEfG6dZ4BpgLd3Fv5NmucncltjGloopYsVHUZcLB8MdDMXW4O7HaXRwOvqmqBqm4DsoGBItIOaKaqK9Q55vMyMCZaMVeVdXIbYxqauBre3j3AhyLyBE6iGuyWJwNf+Twu1y0rdJfLl/slIlNx9kLo2LFjxIIuzzq5jTENTU13cN8J/EZVOwC/AWa75f76IbSScr9U9TlVzVTVzNatW1c72MqkJjdnvXVyG2MaiJpOFhOBN9zlRUBpB3cu0MHncSk4h6hy3eXy5TGXltzMOrmNMQ1GTSeL3cAwd/lS4Ht3+R1gvIgkiEgXnI7slaq6BzgqIoPcUVC3AG/XcMx+paVYJ7cxpuGIWp+FiMwHfga0EpFc4CHgduCvIhIHnMTtX1DVjSKyENgEFAHTVLXYbepOnJFVScD77i3merVrXtbJPaJ321iHY4wxURW1ZKGqEwKs6h/g8Y8Cj/opzwJSIxhaRCQ18nK+TVdujGkg7AzuanA6uY9YJ7cxpt6zZFENzpncBew9UhDrUIwxJqosWVRDmk1XboxpICxZVEOv9s6Z3JYsjDH1nSWLamjcKI7z25xl034YY+o9SxbVVHomtzHG1GeWLKopLbk5eUcL2Gtnchtj6jFLFtWUZtOVG2MaAEsW1WSd3MaYhsCSRTU1bhRH19bWyW2Mqd8sWURAWnJz1lmyMMbUY5YsIiDVOrmNMfWcJYsIsOnKjTH1nSWLCOjVrhlindzGmHrMkkUENEmwTm5jTP1mySJC0uxMbmNMPWbJIkJSk5uz72gB+6yT2xhTD1myiJD0FJuu3BhTf0UtWYjIHBHZJyIbfMoWiMga97ZdRNb4rJshItkiskVERvqU9xeR9e66WSIi0Yq5OqyT2xhTn0Vzz2IuMMq3QFVvUNUMVc0AXgfeABCRXsB4oLdb52kR8brVngGmAt3c2xlt1hbWyW2Mqc+ilixUdRlw0N86d+/gemC+WzQaeFVVC1R1G5ANDBSRdkAzVV2hzoWuXwbGRCvm6rJObmNMfRWrPotLgL2q+r17PxnY6bM+1y1LdpfLl/slIlNFJEtEsvLy8iIccnCpyc3Ze6SAfUetk9sYU7/EKllM4PReBYC/fgitpNwvVX1OVTNVNbN169bVDDF0pdOV26EoY0x9U+PJQkTigGuBBT7FuUAHn/spwG63PMVPea3Uu73byZ17JNahGGNMRMViz+JyYLOq+h5eegcYLyIJItIFpyN7paruAY6KyCC3n+MW4O2aD7lqmiTEcV6rJqzfdTjWoRhjTERFc+jsfGAF0ENEckVkirtqPGcegkJVNwILgU3AB8A0VS12V98JvIDT6Z0DvB+tmCMhLbk5q344xJDHPmV5zv5Yh2OMMREhziCj+iczM1OzsrJqfLsPvLmef3y9A4CkeC+zJ2UyuGurGo/DGGPCISKrVDWzfLmdwR1By3P2s3DV6aNr+YXFTJmbZXsYxpg6z5JFhCzP2c+UuVmcKio5o9wShjGmPrBkESHTF60jv7DY77r8wmKmL1pXwxEZY0zkWLKIkJnj0kmK9/pdlxTvZea49BqOyBhjIseSRYQM7tqK2ZMyKySMxHiPdXIbY+o8SxYR5C9h/HJoV0sUxpg6z5JFhJUmjPYtEmnbLJHF3+2lvg5PNsY0HJYsomBw11Ysv/8yfntFdzbuPsJnW2t+UkNjjImkSpOFiNzkszyk3LpfRSuo+mJM32TaN0/k6SU5sQ7FGGOqJdiexW99lv/fcusmRziWeqdRnIepQ89j5faDrNzm99IexhhTJwRLFhJg2d9948cNAzrSskkjnl6aHetQjDEmbMGShQZY9nff+JHUyMvki7uwdEueXefCGFNnBUsWF4jIOhFZ77Ncer9HDcRXL9x8USeaJsTxzFLruzDG1E1xQdb3rJEo6rlmifHcMrgTTy/NISfvGF1bnxXrkIwxJiSV7lmo6g++N+AY0A9o5d43VXTrkC4kxHls78IYUycFGzr7roikusvtgA04o6BeEZF7oh9e/dHqrATGD+jIW9/uIvfQiViHY4wxIQnWZ9FFVTe4y7cCH6vqz4ELsaGzIZs69DwAnl/27xhHYowxoQmWLAp9li8D3gNQ1aNAid8aJqD2LZK4tl8yr36zk7yjBbEOxxhjqixYstgpIr8WkbE4fRUfAIhIEhBfWUURmSMi+0RkQ7nyX4vIFhHZKCKP+5TPEJFsd91In/L+IrLeXTdLROr0+R13DOvKqeIS5ny5LdahGGNMlQVLFlOA3sAk4AZVPeyWDwJeDFJ3LjDKt0BEhgOjgXRV7Q084Zb3Asa72xoFPC0ipVO3PgNMBbq5tzParGvOa30WV6W145UVP/BTfmHwCsYYUwsEGw21T1XvUNXRqvqRT/kSVX0iSN1lQPk5Lu4EHlPVgtL23fLRwKuqWqCq24BsYKDbqd5MVVeoM3Xry8CYEJ5frTTtZ+dzrKCIV1Zsj3UoxhhTJZWeZyEi71S2XlWvCXF73YFLRORR4CRwr6p+AyQDX/k8LtctK3SXy5cHincqzl4IHTt2DDG0mtOrfTMuvaANc77czuSLu9C4UbDTXYwxJraCfUtdBOwE5gNfU/35oOKAs3EOYw0AForIeQHa1UrK/VLV54DnADIzM2v1dCTThnflF8+sYP7KnUy5uEuswzHGmEoF67NoC/wXkAr8FbgC2K+qn6nqZ2FsLxd4Qx0rcUZUtXLLO/g8LgXY7Zan+Cmv8/p3OocLu5zD88v+TUFRcazDMcaYSgXrsyhW1Q9UdSLO3kA2sFREfh3m9t4CLgUQke5AI2A/8A4wXkQSRKQLTkf2SlXdAxwVkUHuKKhbgLfD3HatM234+fx45CRvrt4V61CMMaZSQa+U536BXwv8A5gGzALeqEK9+cAKoIeI5IrIFGAOcJ47nPZVYKK7l7ERWAhswhmeO01VS39u3wm8gJOocoD3Q3yOtdYl3VqRltycZz7LoajYTlsxxtReUtn1oUXkJZxDUO/jjFbaEPDBtUxmZqZmZWXFOoygPtiwhzv+sZpZE/pyTZ/2sQ7HGNPAicgqVc0sXx5sz+JmnBFMdwPLReSIezsqIkeiEWhDM6JXW7q2bsLTS7KpLHEbY0wsBeuz8KhqU/fWzOfWVFWb1VSQ9ZnHI9z1s/PZ/ONRPt28L3gFY4yJgaB9Fib6rsloT3KLJP5mexfGmFrKkkUtEO/1cMew8/h2x2G++nf5k96NMSb2LFnUEuMyO9DqrAT+viQ71qEYY0wFlixqicR4L7df0oUvsvezZufhWIdjjDFnsGRRi9w4qBPNEuN4ekk2y3P2M+SxT1mesz/WYRljjCWL2uSshDgmDenCR5v2cuuL37DrcD5T5mZZwjDGxJwli1omtb0zIrmgyDmjO7+w2BKGMSbmLFnUIstz9nP3q2sqlFvCMMbEmiWLWmT6onXkF/qfgTa/sJjpi9bVcETGGOOwZFGLzByXTlK81++6xHgPM8el13BExhjjsGRRiwzu2orZkzL9JoxGXg8/nbBrdhtjYsOSRS1TPmEkxXv509g0OpzTmDvnreaueavIO1oQ4yiNMQ2NJYtaqDRhJLdIYvakTCZc2JG3pg1h+sgeLN60jyue/Iy3vt1l80gZY2pMpdezqMvqyvUsQpW97yjTX1vHtzsOc9kFbXh0bBptmyfGOixjTD0R7vUsTC1zfpumvHbHYB64uidf5uzniv/5jAXf7LC9DGNMVFmyqIO8HuG2S87jg7uH0qt9M373+npunr2SnQdPxDo0Y0w9FbVkISJzRGSfe73t0rKHRWSXiKxxb1f5rJshItkiskVERvqU9xeR9e66WSIi0Yq5runcqgnzbx/EH8ek8u2OQ4x8ahkvr9hOSYmzl2HzSxljIiWaexZzgVF+yp9U1Qz39h6AiPQCxgO93TpPi0jp+NFngKlAN/fmr80Gy+MRbh7UiQ9/M5T+nc7mwbc3Mv65r3hjdS5T5mbZ/FLGmIiIWrJQ1WVAVa/kMxp4VVULVHUbkA0MFJF2QDNVXaHOQfmXgTFRCbiOSzm7MS9PHsjj16WzftdhfrtwbdnZ4DZdiDGmumLRZ/ErEVnnHqY62y1LBnb6PCbXLUt2l8uX+yUiU0UkS0Sy8vLyIh13rScipJydhL++bksYxpjqqOlk8QzQFcgA9gB/ccv99UNoJeV+qepzqpqpqpmtW7euZqh10/RF6zjpzlhbns0vZYwJV40mC1Xdq6rFqloCPA8MdFflAh18HpoC7HbLU/yUmwAqm18qKd5r80sZY8JSo8nC7YMoNRYoHSn1DjBeRBJEpAtOR/ZKVd0DHBWRQe4oqFuAt2sy5rom0PxSSfFeZk/KZHDXVjGKzBhTl0Vz6Ox8YAXQQ0RyRWQK8Lg7DHYdMBz4DYCqbgQWApuAD4Bpqlo6V/edwAs4nd45wPvRirm+KJ8wBHjhFksUxpjw2XQf9djynP1Mm7eaQycKWfjLixjY5ZxYh2SMqeVsuo8GaHDXVnzxu0tJivfy5re5wSsYY0wAlizquSYJcYxKbcu76/ZwMsBV+IwxJhhLFg3AmL7JHD1ZxJLN+2IdijGmjrJk0QAM6dqS1k0TeOPbXbEOxRhTR1myaADivB5G92nP0i37OHT8VKzDMcbUQZYsGogxfZMpLFbeXb8n1qEYY+ogSxYNRO/2zeh+7lm8udpGRRljQmfJooEQEcb2TWH1jsNs33881uEYY+oYSxYNyOiM9ojAW2uso9sYExpLFg1I+xZJDOrSkje/3WXX7DbGhMSSRQMztl8yPxw4weodh2MdijGmDrFk0cBcmdqWhDiPTf9hjAmJJYsGpmliPCN6O9N/nApwkSRjjCnPkkUDNLZvew6fKGTpFpv+wxhTNZYsGqBLurWmZZNGvGnTfxhjqsiSRQMU7/Xw8z7t+eS7ffyUXxjrcIwxdYAliwZqbN9kThWX8J5N/2GMqQJLFg1UekpzzmvdhDdX26EoY0xwliwaKBHh2r7JrNx+kJ0HT8Q6HGNMLRe1ZCEic0Rkn4hs8LPuXhFREWnlUzZDRLJFZIuIjPQp7y8i6911s0REohVzQzM6IxmAt236D2NMENHcs5gLjCpfKCIdgCuAHT5lvYDxQG+3ztMi4nVXPwNMBbq5twptmvB0OKcxAzufwxs2/YcxJoioJQtVXQYc9LPqSeA+wPfbaTTwqqoWqOo2IBsYKCLtgGaqukKdb7OXgTHRirkhGtsvmX/nHWdd7k+xDsUYU4vVaJ+FiFwD7FLVteVWJQM7fe7numXJ7nL58kDtTxWRLBHJysvLi1DU9dtVae1oFOexcy6MMZWqsWQhIo2B3wMP+lvtp0wrKfdLVZ9T1UxVzWzdunV4gTYwzZPiubxnG/65djeFxTb9hzHGv5rcs+gKdAHWish2IAVYLSJtcfYYOvg8NgXY7Zan+Ck3ETQmI5kDx0/x+fe2N2aM8a/GkoWqrlfVNqraWVU74ySCfqr6I/AOMF5EEkSkC05H9kpV3QMcFZFB7iioW4C3ayrmhuJnPdpwduN43rBzLowxAURz6Ox8YAXQQ0RyRWRKoMeq6kZgIbAJ+ACYpqrF7uo7gRdwOr1zgPejFXND1SjOw3+kt+fjTXs5etKm/zDGVBQXrYZVdUKQ9Z3L3X8UeNTP47KA1IgGZyoY0zeZV776gfc3/Mj1mR2CVzDGNCh2BrcBoF/HFnRu2dim/zDG+GXJwgDO9B9j+ibz1bYD7D6cH+twjDG1jCULU2Zs32RU4e01NuDMGHMmSxamTKeWTejXsQVvfptr038YY85gycKcYWy/FLbuPcbG3UdiHYoxphaxZGHO8B9p7Yj3Cm/Z9B/GGB+WLMwZzm7SiOE92vD22t0U2fQfxhiXJQtTwdi+yeQdLeDLnAOxDsUYU0tYsjAVXNqzDc0S4+xQlDGmjCULU0FCnJer09vzwYYf+XTzXoY89inLc/bHOixjTAxZsjB+je2bTH5hMb98ZRW7DuczZW6WJQxjGjBLFsavwqISBCgsds63yC8stoRhTANmycJUsDxnP7e9nFXhKlOWMIxpuCxZmAqmL1pHfmGx33X5hcVMX7SuhiMyxsSaJQtTwcxx6STFe/2ua+T1MHNceg1HZIyJNUsWpoLBXVsxe1JmhYQhwKniEl5e/gO7bGZaYxoUSxbGr/IJIyney9zJA5g+sgdLt+7jsr8s5e9Lsiko8n+4yhhTv1iyMAGVJozkFknMnpTJsO5tmDb8fD75vz/jZ93bMPPDLYx66nOWbc2LdajGmCiL5jW454jIPhHZ4FP2RxFZJyJrROQjEWnvs26GiGSLyBYRGelT3l9E1rvrZomIRCtmU9Hgrq348v5LGdy1VVlZcosknr25Py9NHgjALXNWcuc/VtmhKWPqsWjuWcwFRpUrm6mq6aqaAbwLPAggIr2A8UBvt87TIlJ6wPwZYCrQzb2Vb9PEyLDurfngnkuYPrIHS7bs4/K/fMbTS7M5VXR6AsLlOfvtDHBj6oGoJQtVXQYcLFfme5GEJlA2lH808KqqFqjqNiAbGCgi7YBmqrpCnavxvAyMiVbMJnQJcV6mDT+fxb8dxtDurXj8gy2M+usyPv8+j+U5+5kyN8vOADemHqjxPgsReVREdgI34u5ZAMnATp+H5bplye5y+fJAbU8VkSwRycrLs+PoNSnl7Mb8fzdn8uKtAygpUW6evZKbXvi67HwNO6HPmLqtxpOFqv5eVTsA84BfucX++iG0kvJAbT+nqpmqmtm6devqB2tCNrxHGx6+pjdxHqGk3H/KEoYxdVcsR0P9L/ALdzkX6OCzLgXY7Zan+Ck3tdjv39xAUflM4covLOaeV9fYNb6NqWNqNFmISDefu9cAm93ld4DxIpIgIl1wOrJXquoe4KiIDHJHQd0CvF2TMZvQVXYGOMC+owUMf2Ipj/5rE1//+4Bdkc+YOiCaQ2fnAyuAHiKSKyJTgMdEZIOIrANGAHcDqOpGYCGwCfgAmKaqpWd73Qm8gNPpnQO8H62YTWQEOgM8Kd7L3/6zL38ck0qnlk14afkP3PDcVwx4dDG/XbiG99fv4XhBUYX2IjGiykZlGVM9Ul8PB2RmZmpWVlasw2jQSkdD5RcWkxTvZfakzDPO1zhWUMSyrXks3rSXT7fs4/CJQhp5PQw+vyWX9zyXK3qdS07esUrbiEQcJjwHDsCNN8K8edCyZayjMZEiIqtUNbNCuSULE03Lc/YzfdE6Zo5Lr/QLuqi4hKwfDvHxpr18vGkvOw6eAEAEfN+iifEenr8lk0u6VW0Ag2+iKGUJIzL+8he4917n729/G+toTKRYsjB1hqqyaFUu//XG+oAd5fEeoXnjeM5KiKNJQhxnJcTRNDHu9P3EOA4dP8Wb3+4qu4CTr3ASRlUTX7TVhjhUoUMH2LULUlJgxw4nsZu6L1CysLmhTK0jIvx18fcBEwVAQryXEb3b0qdDC9o2S0SB3YdP8u3Ow3yw4Ude/HI7C7Ny/SYKcEZl3fZSFs8v+zdLtuxj58ETlFSyvUidYFjdvpPaEsfnn8PBQ87rdeCg8sUXYTVTa/qjIvF/iXUMkWojENuzMLWSv8NHpaq6V7Bsax5TX8niZGHF0VYegbMS4jhy8nSHelK8l/NaN6Fbm7M43+e256eTTH15VbUPZVW37yRSh9Qi0YczbEQByz6Ox/m9WcLQKwr57KOEkNqIRBy1oY3aEEOk2gA7DGXqoEh8OQZr49DxU2TnHeP7vcfI3neM7Lxj5Ow7VqVJEeM8wvWZKZzfpinxXiHe63FucR7iPXJ62Sts+fEoj72/mQKfebMS4jw8cHVPeic3R1UpLoHiEnWWVSlRKClRSlTZuPsIf/s0m1N+hhk38nq4d2R30lNaEO8V4jwevO7247xCnEeI83qI8wjf7jjEPQvWnJFAE+M9zJk0IOBrOno0vPNOuUJvCRR7At8HrrkG3g4w0L0m/rc10UZtiCFSbZSyZGHqpFj94jpeUERO3jEmzlnJoROF1XkKdYZHIM7jweMBrwgej+D1CIV5Z5H9Sl8KjzdCiwKfP1PWTlwx8U1P0XvSOhq3PY7q6UEKqsrJomIOHg/8mjZLjKdRnIfSyRrK6nK6jcLiEo4VBL6WSpNGXuK8p5OXv++5ouISTvjZ6yyVFO8hzuM5Y8oI33aKS5STRYHrJ8Q5SbsyxSV6xg+ISLcRzmfGkoWpsyLRoRtuG5UdDkuM9/DMjf3o1+kcCotLnFuRcqq4hKKS08u/fCWL/cdOBdxGyyaNePKGDLweQeT0F7VHBI+A1yNs2PUTf/jnJr9fCglxHu4b1YOebZtRWKIUl5RQWKwUFStFJSUUFSvFJcr/8953HM4P/CXdNCGOGwd1okSdxxe7ezVFJcrJE/De31PYmtUULYwL2IbEF3F+5hEun7qTRokliIgzZ4+A4Dy/99bv4cSpwF/0jRt5GdPXmQKu9GuytPPcbY03VudyvJI2miR4Gde/Q8D1AItW7eR4ZQknwcsNmR3P2L5vTK9+s6PShHVWgpcJAztWGsP8ldFvI7lFEl/ef2mlbfiyZGFMmKJxqCJS7dR0HPf98ShP/KExWlxxD0O8xdz70Ake/++mUY+jNrRRG2KIVBu+bDSUMWHyd4nZUD6AlZ3RXp12YhHHdSOb0jjJ/2GRxknCuFGVJ4pIxVEb2qgNMUSqjaqwZGFMFZS/xGyoH8DqftHXljiysqCkrCNbkbgiSnsTSoo9VHVnPhKvR21oozbEEKk2glLVennr37+/GlPbfJmdp4P/9Il+mZ1XJ+MYP97prk5MVG2bXKQ9blqr57Yv0sREp3zChJqJo7a1URtiiFQbQJb6+U61PgtjTJWdd55ztvZ118Hs2dCkCRw/DpMnw+uvQ6dOkJMT6yhNdVifhTGm2nr2hOeeg1dfdRIFOH8XLHDKL7ggtvGZ6LE9C2OMMWVsz8IYY0zYLFkYY4wJqt4ehhKRPOCHMKu3Aqo7baO1UbtisDasjWi3URtiiEQbnVS1wgVj6m2yqA4RyfJ3zM7aqLsxWBvWRrTbqA0xRKoNf+wwlDHGmKAsWRhjjAnKkoV/z1kbEW2jNsRgbVgb0W6jNsQQqTYqsD4LY4wxQdmehTHGmKAsWRhjjAnKkoUPEZkjIvtEZEOY9TuIyBIR+U5ENorI3WG0kSgiK0VkrdvGH8KJxW3LKyLfisi7YdbfLiLrRWSNiIQ1d4qItBCR10Rks/u6XBRi/R7u9ktvR0TknjDi+I37em4QkfkikhhGG3e79TdWNQZ/7ykROUdEPhaR792/Z4fRxjg3jhIRCTpMMkAbM93/yzoReVNEWoTRxh/d+mtE5CMRaR9KfZ9194qIikilc2oHiOFhEdnl8x65KtQ23PJfi8gW93V9PIw4FvjEsF1E1oTRRoaIfFX6mRORgWG00UdEVrif3X+KSLPK2qgyf1PRNtQbMBToB2wIs347oJ+73BTYCvQKsQ0BznKX44GvgUFhxvNb4H+Bd8Osvx1oVc3X9CXgNne5EdCiGm15gR9xThoKpV4ysA1Icu8vBCaF2EYqsAFoDMQBi4Fu4byngMeB+93l+4E/h9FGT6AHsBTIDDOOEUCcu/znMONo5rP8f4BnQ6nvlncAPsQ5ibbS91uAGB4G7g3hf+mvjeHu/zTBvd8m1DbKrf8L8GAYcXwEXOkuXwUsDaONb4Bh7vJk4I+hvNcD3WzPwoeqLgMOVqP+HlVd7S4fBb7D+aIKpQ1V1WPu3Xj3FvIoBBFJAa4GXgi1bqS4v2iGArMBVPWUqh6uRpOXATmqGs6Z+XFAkojE4Xzh7w6xfk/gK1U9oapFwGfA2GCVArynRuMkUdy/Y0JtQ1W/U9UtVQs9YBsfuc8F4CsgJYw2jvjcbUIl79VKPl9PAvdVVrcKbVRZgDbuBB5T1QL3MfvCjUNEBLgemB9GGwqU7gk0J8j7NEAbPYBl7vLHwC8qa6OqLFlEiYh0Bvri7BmEWtfr7sLuAz5W1ZDbAJ7C+QCWhFG3lAIficgqEZkaRv3zgDzgRfdw2Asi0qQa8YwnyAfQH1XdBTwB7AD2AD+p6kchNrMBGCoiLUWkMc6vvg6hxuI6V1X3uLHtAdqE2U4kTQbeD6eiiDwqIjuBG4EHQ6x7DbBLVdeGs20fv3IPh80JdlgvgO7AJSLytYh8JiIDqhHLJcBeVf0+jLr3ADPd1/MJYEYYbWwArnGXxxH++/QMliyiQETOAl4H7in3y6tKVLVYVTNwfukNFJHUELf/H8A+VV0V6rbLGaKq/YArgWkiMjTE+nE4u8jPqGpf4DjOYZeQiUgjnA/AojDqno3za74L0B5oIiI3hdKGqn6Hc6jmY+ADYC1QVGmlOkJEfo/zXOaFU19Vf6+qHdz6vwphu42B3xNigvHjGaArkIHzY+AvYbQRB5wNDAKmAwvdPYRwTCCMHzWuO4HfuK/nb3D3ykM0GefzugrncPipMGM5gyWLCBOReJxEMU9V36hOW+4hm6XAqBCrDgGuEZHtwKvApSLyjzC2v9v9uw94E6i0s82PXCDXZ8/oNZzkEY4rgdWqujeMupcD21Q1T1ULgTeAwaE2oqqzVbWfqg7F2fUP55cjwF4RaQfg/q30kEc0ichE4D+AG9U9yF0N/0tohzy64iTwte57NQVYLSJtQ9moqu51f2CVAM8T+vsUnPfqG+5h4JU4e+QhX8DaPcx5LbAgjBgAJuK8P8H5YRTyc1HVzao6QlX74yStiFy70JJFBLm/RGYD36nq/4TZRuvSUSkikoTzRbc5lDZUdYaqpqhqZ5xDN5+qaki/pEWkiYg0LV3G6QwNaZSYqv4I7BSRHm7RZcCmUNrwUZ1fazuAQSLS2P0fXYbTnxQSEWnj/u2I84UQbjzv4Hwp4P59O8x2qkVERgG/A65R1RNhttHN5+41hPBeVdX1qtpGVTu779VcnAEiP4YYQzufu2MJ8X3qegu41G2vO85gjHBmbr0c2KyquWHUBaePYpi7fClh/CDxeZ96gAeAZ8OM5UyR6CWvLzecD/8eoBDnjTslxPoX4xznXwescW9XhdhGOvCt28YGgoyoqEJ7PyOM0VA4/Q1r3dtG4Pdhbj8DyHKfz1vA2WG00Rg4ADSvxuvwB5wvsg3AK7ijXkJs43OcZLcWuCzc9xTQEvgE54vgE+CcMNoY6y4XAHuBD8NoIxvY6fNeDTiSqZI2Xndf03XAP4HkUOqXW7+d4KOh/MXwCrDejeEdoF0YbTQC/uE+l9XApaG24ZbPBe6oxnvjYmCV+x77GugfRht344zE3Ao8hjtTR3VvNt2HMcaYoOwwlDHGmKAsWRhjjAnKkoUxxpigLFkYY4wJypKFMcaYoCxZGBMGEfmTiPxMRMaISEhnpbvn0nztToFySbl1S92ZT9eKyJel56iISLyIPCbOTLUbxJmZ+MpIPidjKmPJwpjwXIgzDn4YzvkXobgM58Stvqrqr+6NqtoHZ5LBmW7ZH3FmNU5V1VTg5zhTORhTI+w8C2NCICIzgZE401Tk4ExZsQ14TVUfKffYTsAcoDXOhIq3AufgnDiWBOwCLlLVfJ86S3Gm284SkQtwpn7IxDl5rouGMdeYMZFgexbGhEBVpwO34ZypOwBYp6rp5ROF62/Ay6qajjPJ3ixVXYMzcd4CVc3wTRR+/BznzOTzgR2WKEwsxcU6AGPqoL4402NcQOVzXV2EM4cUOFNSVHr1NR/zRCQfZ/qLX+PMhmpMTFmyMKaKRCQDZ48iBWeSucZOsayh3OGkAKp6zPdGVS27jK2IHAA6ikhTdS6qZUyNs8NQxlSRqq5R5zojW4FewKfAyEoOJy3HmfUXnAsDfRHmdk/gzGY8y72uByLSLtRrchhTHZYsjAmBiLQGDqlz7YQLVLWyw1D/B7hVRNYBN+PMBhquB3A6yTeJyAacGXzzqtGeMSGx0VDGGGOCsj0LY4wxQVmyMMYYE5QlC2OMMUFZsjDGGBOUJQtjjDFBWbIwxhgTlCULY4wxQf3/JirylziQp44AAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.cross_decomposition import PLSRegression\n",
    "\n",
    "# 10-fold CV\n",
    "n = len(X_train)\n",
    "\n",
    "mse = []\n",
    "\n",
    "for i in np.arange(1, 20):\n",
    "    pls = PLSRegression(n_components=i)\n",
    "    score = -1 * cross_val_score(pls, scale(X_train), y_train, cv=kf, scoring='neg_mean_squared_error').mean()\n",
    "    mse.append(np.sqrt(score))\n",
    "\n",
    "min_mse = np.array(mse).argmin()+1\n",
    "    \n",
    "print(f\"M = {min_mse}\")\n",
    "plt.plot(range(1,20), mse, '-D')\n",
    "plt.xlabel('# of PC')\n",
    "plt.ylabel('MSE')\n",
    "plt.xticks(range(1,20), range(1,20))\n",
    "min_mse_marker, = plt.plot(min_mse, min(mse), 'b*', markersize=15)\n",
    "plt.legend([min_mse_marker], ['Best number of principal components']);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test error = 829.1363522415386\n",
      "r2 = 0.9528400055708414\n"
     ]
    }
   ],
   "source": [
    "pls = PLSRegression(n_components=13)\n",
    "pls.fit(scale(X_train), y_train)\n",
    "print(f\"test error = {np.sqrt(mean_squared_error(pls.predict(scale(X_test)), y_test))}\")\n",
    "print(f\"r2 = {r2_score(pls.predict(scale(X_test)), y_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(g) Comment on the results obtained. How accurately can we predict the number of college applications received? Is there much difference among the test errors resulting from these five approaches?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can compare the predictions of the models by comparing their test errors and their r2 values. All the models, except PCR, have r2 value of approximately 0.95 and comparable test errors of over 800. The lasso model has the lowest test error (824.76) and highest r2 value (0.9539). The PCR model has a abnormally low r2 of 0.66 and abnormally high test error of 2117."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. (35pt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have seen that as the number of features used in a model increases, the training error will necessarily decrease, but the test error may not. We will now explore this in a simulated data set.\n",
    "\n",
    "**(a) Generate a data set with $p=20$ features, $n=1,000$ observations, and an associated quantitative response vector generated according to the model**\n",
    "$$\n",
    "Y=X \\beta+\\epsilon\n",
    "$$\n",
    "**where $\\beta$ has some elements that are exactly equal to zero (No limits on the number of zeros).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(2002)\n",
    "e = np.random.rand(1000,1)\n",
    "X = np.random.rand(1000, 20)\n",
    "beta = np.multiply(np.random.rand(20,1), np.random.randint(5, size=(20,1)))\n",
    "y = np.matmul(X,beta)+e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(b) Split your data set into a training set containing 100 observations and a test set containing 900 observations.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>predictor_0</th>\n",
       "      <th>predictor_1</th>\n",
       "      <th>predictor_2</th>\n",
       "      <th>predictor_3</th>\n",
       "      <th>predictor_4</th>\n",
       "      <th>predictor_5</th>\n",
       "      <th>predictor_6</th>\n",
       "      <th>predictor_7</th>\n",
       "      <th>predictor_8</th>\n",
       "      <th>predictor_9</th>\n",
       "      <th>...</th>\n",
       "      <th>predictor_11</th>\n",
       "      <th>predictor_12</th>\n",
       "      <th>predictor_13</th>\n",
       "      <th>predictor_14</th>\n",
       "      <th>predictor_15</th>\n",
       "      <th>predictor_16</th>\n",
       "      <th>predictor_17</th>\n",
       "      <th>predictor_18</th>\n",
       "      <th>predictor_19</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.236229</td>\n",
       "      <td>0.413622</td>\n",
       "      <td>0.984802</td>\n",
       "      <td>0.305039</td>\n",
       "      <td>0.000709</td>\n",
       "      <td>0.079842</td>\n",
       "      <td>0.235952</td>\n",
       "      <td>0.841108</td>\n",
       "      <td>0.798813</td>\n",
       "      <td>0.065558</td>\n",
       "      <td>...</td>\n",
       "      <td>0.623158</td>\n",
       "      <td>0.215144</td>\n",
       "      <td>0.800383</td>\n",
       "      <td>0.693093</td>\n",
       "      <td>0.382382</td>\n",
       "      <td>0.950773</td>\n",
       "      <td>0.015821</td>\n",
       "      <td>0.021364</td>\n",
       "      <td>0.677080</td>\n",
       "      <td>11.061506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.113131</td>\n",
       "      <td>0.911335</td>\n",
       "      <td>0.675314</td>\n",
       "      <td>0.671790</td>\n",
       "      <td>0.354549</td>\n",
       "      <td>0.709681</td>\n",
       "      <td>0.310553</td>\n",
       "      <td>0.420827</td>\n",
       "      <td>0.742953</td>\n",
       "      <td>0.704164</td>\n",
       "      <td>...</td>\n",
       "      <td>0.625502</td>\n",
       "      <td>0.679528</td>\n",
       "      <td>0.375318</td>\n",
       "      <td>0.905669</td>\n",
       "      <td>0.036683</td>\n",
       "      <td>0.902423</td>\n",
       "      <td>0.709506</td>\n",
       "      <td>0.650090</td>\n",
       "      <td>0.602260</td>\n",
       "      <td>14.807274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.209964</td>\n",
       "      <td>0.588460</td>\n",
       "      <td>0.860698</td>\n",
       "      <td>0.010839</td>\n",
       "      <td>0.384141</td>\n",
       "      <td>0.969902</td>\n",
       "      <td>0.342374</td>\n",
       "      <td>0.104217</td>\n",
       "      <td>0.562591</td>\n",
       "      <td>0.128669</td>\n",
       "      <td>...</td>\n",
       "      <td>0.503853</td>\n",
       "      <td>0.812095</td>\n",
       "      <td>0.037219</td>\n",
       "      <td>0.955049</td>\n",
       "      <td>0.677874</td>\n",
       "      <td>0.598056</td>\n",
       "      <td>0.478735</td>\n",
       "      <td>0.024186</td>\n",
       "      <td>0.692503</td>\n",
       "      <td>13.416345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.836917</td>\n",
       "      <td>0.826800</td>\n",
       "      <td>0.933058</td>\n",
       "      <td>0.788189</td>\n",
       "      <td>0.743407</td>\n",
       "      <td>0.119242</td>\n",
       "      <td>0.863755</td>\n",
       "      <td>0.668830</td>\n",
       "      <td>0.365146</td>\n",
       "      <td>0.820229</td>\n",
       "      <td>...</td>\n",
       "      <td>0.421597</td>\n",
       "      <td>0.605519</td>\n",
       "      <td>0.297890</td>\n",
       "      <td>0.405773</td>\n",
       "      <td>0.204024</td>\n",
       "      <td>0.982388</td>\n",
       "      <td>0.351100</td>\n",
       "      <td>0.171241</td>\n",
       "      <td>0.238894</td>\n",
       "      <td>17.526264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.503628</td>\n",
       "      <td>0.623269</td>\n",
       "      <td>0.947238</td>\n",
       "      <td>0.273395</td>\n",
       "      <td>0.901803</td>\n",
       "      <td>0.402843</td>\n",
       "      <td>0.394938</td>\n",
       "      <td>0.887377</td>\n",
       "      <td>0.875102</td>\n",
       "      <td>0.412057</td>\n",
       "      <td>...</td>\n",
       "      <td>0.752421</td>\n",
       "      <td>0.340500</td>\n",
       "      <td>0.074998</td>\n",
       "      <td>0.600390</td>\n",
       "      <td>0.724420</td>\n",
       "      <td>0.425409</td>\n",
       "      <td>0.372130</td>\n",
       "      <td>0.494372</td>\n",
       "      <td>0.915367</td>\n",
       "      <td>15.757745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>0.341821</td>\n",
       "      <td>0.216339</td>\n",
       "      <td>0.529831</td>\n",
       "      <td>0.766525</td>\n",
       "      <td>0.469650</td>\n",
       "      <td>0.604889</td>\n",
       "      <td>0.161910</td>\n",
       "      <td>0.412209</td>\n",
       "      <td>0.347930</td>\n",
       "      <td>0.278566</td>\n",
       "      <td>...</td>\n",
       "      <td>0.912951</td>\n",
       "      <td>0.751329</td>\n",
       "      <td>0.207496</td>\n",
       "      <td>0.108247</td>\n",
       "      <td>0.063734</td>\n",
       "      <td>0.481007</td>\n",
       "      <td>0.299111</td>\n",
       "      <td>0.347433</td>\n",
       "      <td>0.111436</td>\n",
       "      <td>12.483033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>0.635508</td>\n",
       "      <td>0.231310</td>\n",
       "      <td>0.156352</td>\n",
       "      <td>0.690118</td>\n",
       "      <td>0.787922</td>\n",
       "      <td>0.088311</td>\n",
       "      <td>0.313683</td>\n",
       "      <td>0.817012</td>\n",
       "      <td>0.922009</td>\n",
       "      <td>0.409334</td>\n",
       "      <td>...</td>\n",
       "      <td>0.040255</td>\n",
       "      <td>0.046179</td>\n",
       "      <td>0.702802</td>\n",
       "      <td>0.542310</td>\n",
       "      <td>0.665636</td>\n",
       "      <td>0.665287</td>\n",
       "      <td>0.567038</td>\n",
       "      <td>0.363528</td>\n",
       "      <td>0.341921</td>\n",
       "      <td>12.965482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>0.620660</td>\n",
       "      <td>0.682598</td>\n",
       "      <td>0.825929</td>\n",
       "      <td>0.896867</td>\n",
       "      <td>0.948431</td>\n",
       "      <td>0.846491</td>\n",
       "      <td>0.989122</td>\n",
       "      <td>0.951765</td>\n",
       "      <td>0.042126</td>\n",
       "      <td>0.403444</td>\n",
       "      <td>...</td>\n",
       "      <td>0.172645</td>\n",
       "      <td>0.424250</td>\n",
       "      <td>0.442831</td>\n",
       "      <td>0.615957</td>\n",
       "      <td>0.085489</td>\n",
       "      <td>0.268074</td>\n",
       "      <td>0.605560</td>\n",
       "      <td>0.995201</td>\n",
       "      <td>0.957008</td>\n",
       "      <td>20.738617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>0.036644</td>\n",
       "      <td>0.843330</td>\n",
       "      <td>0.241407</td>\n",
       "      <td>0.530007</td>\n",
       "      <td>0.805514</td>\n",
       "      <td>0.708195</td>\n",
       "      <td>0.563627</td>\n",
       "      <td>0.291633</td>\n",
       "      <td>0.236084</td>\n",
       "      <td>0.262836</td>\n",
       "      <td>...</td>\n",
       "      <td>0.676281</td>\n",
       "      <td>0.073989</td>\n",
       "      <td>0.508269</td>\n",
       "      <td>0.891776</td>\n",
       "      <td>0.079466</td>\n",
       "      <td>0.411819</td>\n",
       "      <td>0.625804</td>\n",
       "      <td>0.281945</td>\n",
       "      <td>0.973839</td>\n",
       "      <td>10.384073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>0.500756</td>\n",
       "      <td>0.484629</td>\n",
       "      <td>0.884475</td>\n",
       "      <td>0.498940</td>\n",
       "      <td>0.269876</td>\n",
       "      <td>0.136176</td>\n",
       "      <td>0.157340</td>\n",
       "      <td>0.786603</td>\n",
       "      <td>0.588118</td>\n",
       "      <td>0.371835</td>\n",
       "      <td>...</td>\n",
       "      <td>0.117702</td>\n",
       "      <td>0.547701</td>\n",
       "      <td>0.909893</td>\n",
       "      <td>0.948533</td>\n",
       "      <td>0.470327</td>\n",
       "      <td>0.039030</td>\n",
       "      <td>0.448264</td>\n",
       "      <td>0.809361</td>\n",
       "      <td>0.298448</td>\n",
       "      <td>14.622905</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows Ã— 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     predictor_0  predictor_1  predictor_2  predictor_3  predictor_4  \\\n",
       "0       0.236229     0.413622     0.984802     0.305039     0.000709   \n",
       "1       0.113131     0.911335     0.675314     0.671790     0.354549   \n",
       "2       0.209964     0.588460     0.860698     0.010839     0.384141   \n",
       "3       0.836917     0.826800     0.933058     0.788189     0.743407   \n",
       "4       0.503628     0.623269     0.947238     0.273395     0.901803   \n",
       "..           ...          ...          ...          ...          ...   \n",
       "995     0.341821     0.216339     0.529831     0.766525     0.469650   \n",
       "996     0.635508     0.231310     0.156352     0.690118     0.787922   \n",
       "997     0.620660     0.682598     0.825929     0.896867     0.948431   \n",
       "998     0.036644     0.843330     0.241407     0.530007     0.805514   \n",
       "999     0.500756     0.484629     0.884475     0.498940     0.269876   \n",
       "\n",
       "     predictor_5  predictor_6  predictor_7  predictor_8  predictor_9  ...  \\\n",
       "0       0.079842     0.235952     0.841108     0.798813     0.065558  ...   \n",
       "1       0.709681     0.310553     0.420827     0.742953     0.704164  ...   \n",
       "2       0.969902     0.342374     0.104217     0.562591     0.128669  ...   \n",
       "3       0.119242     0.863755     0.668830     0.365146     0.820229  ...   \n",
       "4       0.402843     0.394938     0.887377     0.875102     0.412057  ...   \n",
       "..           ...          ...          ...          ...          ...  ...   \n",
       "995     0.604889     0.161910     0.412209     0.347930     0.278566  ...   \n",
       "996     0.088311     0.313683     0.817012     0.922009     0.409334  ...   \n",
       "997     0.846491     0.989122     0.951765     0.042126     0.403444  ...   \n",
       "998     0.708195     0.563627     0.291633     0.236084     0.262836  ...   \n",
       "999     0.136176     0.157340     0.786603     0.588118     0.371835  ...   \n",
       "\n",
       "     predictor_11  predictor_12  predictor_13  predictor_14  predictor_15  \\\n",
       "0        0.623158      0.215144      0.800383      0.693093      0.382382   \n",
       "1        0.625502      0.679528      0.375318      0.905669      0.036683   \n",
       "2        0.503853      0.812095      0.037219      0.955049      0.677874   \n",
       "3        0.421597      0.605519      0.297890      0.405773      0.204024   \n",
       "4        0.752421      0.340500      0.074998      0.600390      0.724420   \n",
       "..            ...           ...           ...           ...           ...   \n",
       "995      0.912951      0.751329      0.207496      0.108247      0.063734   \n",
       "996      0.040255      0.046179      0.702802      0.542310      0.665636   \n",
       "997      0.172645      0.424250      0.442831      0.615957      0.085489   \n",
       "998      0.676281      0.073989      0.508269      0.891776      0.079466   \n",
       "999      0.117702      0.547701      0.909893      0.948533      0.470327   \n",
       "\n",
       "     predictor_16  predictor_17  predictor_18  predictor_19          y  \n",
       "0        0.950773      0.015821      0.021364      0.677080  11.061506  \n",
       "1        0.902423      0.709506      0.650090      0.602260  14.807274  \n",
       "2        0.598056      0.478735      0.024186      0.692503  13.416345  \n",
       "3        0.982388      0.351100      0.171241      0.238894  17.526264  \n",
       "4        0.425409      0.372130      0.494372      0.915367  15.757745  \n",
       "..            ...           ...           ...           ...        ...  \n",
       "995      0.481007      0.299111      0.347433      0.111436  12.483033  \n",
       "996      0.665287      0.567038      0.363528      0.341921  12.965482  \n",
       "997      0.268074      0.605560      0.995201      0.957008  20.738617  \n",
       "998      0.411819      0.625804      0.281945      0.973839  10.384073  \n",
       "999      0.039030      0.448264      0.809361      0.298448  14.622905  \n",
       "\n",
       "[1000 rows x 21 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_df = pd.DataFrame(X, columns = [f\"predictor_{i}\" for i in range(20)])\n",
    "y_df = pd.DataFrame(y, columns = [\"y\"])\n",
    "df = pd.concat([x_df,y_df], axis=1)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(x_df, y_df, test_size=0.9, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(c) Perform best subset selection on the training set, and plot the training set MSE associated with the best model of each size (You may use the code from Lab 5) (Run as many number of features as your PC allows.).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm # a python package that provides progress bars for iterables\n",
    "from itertools import combinations\n",
    "from operator import itemgetter\n",
    "import statsmodels.api as sm\n",
    "def best_subsets(dataframe, predictors, response, max_features=8):\n",
    "    \"\"\"\n",
    "    Regresses response onto subsets of the predictors in dataframe. Compares models with equal feature \n",
    "    numbers choosing the one with the lowest RSS as the 'best' model for that number of features.\n",
    "    \n",
    "    PARAMETERS\n",
    "    ----------\n",
    "    dataframe : pandas dataframe obj containing responses and predictors\n",
    "    predictors : list of column names of dataframe used as features\n",
    "    response : list of column name of dataframe used as target\n",
    "    \n",
    "    RETURNS\n",
    "    -------\n",
    "    a list of best models, one per feature number\n",
    "    \n",
    "    ex.\n",
    "    [best 1 feat model, best two feat model] = best_subsets(df, predictors, response, max_features = 2)\n",
    "    \"\"\"\n",
    "    \n",
    "    def process_linear_model(features):\n",
    "        \"\"\"\n",
    "        Constructs Linear Regression Model of response onto features.\n",
    "        \"\"\"\n",
    "        # Create design Matrix\n",
    "        X = sm.add_constant(dataframe[features])\n",
    "        y = dataframe[response]\n",
    "\n",
    "        model = sm.OLS(y,X).fit()\n",
    "        RSS = model.ssr\n",
    "        return (model, RSS)\n",
    "\n",
    "    def get_best_kth_model(k):\n",
    "        \"\"\"\n",
    "        Returns the model from all models with k-predictors with the lowest RSS.\n",
    "        \"\"\"\n",
    "        results = []\n",
    "\n",
    "        for combo in combinations(predictors, k):\n",
    "            # process linear model with this combo of features\n",
    "            results.append(process_linear_model(list(combo)))\n",
    "\n",
    "        # sort the models and return the one with the smallest RSS\n",
    "        return sorted(results, key=itemgetter(1)).pop(0)[0]\n",
    "    \n",
    "    models =[]\n",
    "    for k in tqdm(range(1,max_features+1)):\n",
    "        models.append(get_best_kth_model(k))\n",
    "    \n",
    "    return models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:43<00:00,  8.64s/it]\n"
     ]
    }
   ],
   "source": [
    "predictors = list(X_train.columns)\n",
    "models = best_subsets(df, predictors, ['y'], max_features=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(f) How does the model at which the test set MSE is minimized compare to the true model used to generate the data? Comment on the coefficient values.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "const           6.243559\n",
      "predictor_0     3.464717\n",
      "predictor_2     3.315681\n",
      "predictor_3     2.705245\n",
      "predictor_6     2.587181\n",
      "predictor_10    3.192922\n",
      "dtype: float64\n",
      "[[3.3233]\n",
      " [0.4415]\n",
      " [3.4706]\n",
      " [2.5873]\n",
      " [1.7095]\n",
      " [1.0666]\n",
      " [2.6614]\n",
      " [0.    ]\n",
      " [0.3393]\n",
      " [0.6786]\n",
      " [3.3858]\n",
      " [1.1098]\n",
      " [0.6567]\n",
      " [0.3652]\n",
      " [0.9809]\n",
      " [0.1272]\n",
      " [0.    ]\n",
      " [1.0579]\n",
      " [2.0073]\n",
      " [0.6231]]\n"
     ]
    }
   ],
   "source": [
    "print(models[4].params)\n",
    "print(beta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the derived beta values are quite close to the original values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(g) Create a plot displaying $\\sqrt{\\sum_{j=1}^{p}\\left(\\beta_{j}-\\hat{\\beta}_{j}^{r}\\right)^{2}}$ for a range of values of $r$, where $\\hat{\\beta}_{j}^{r}$ is the $j$ th coefficient estimate for the best model containing $r$ coefficients. Comment on what you observe. How does this compare to the test MSE plot from (d)?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "const          12.083085\n",
      "predictor_0     3.519761\n",
      "dtype: float64\n",
      "const           10.336389\n",
      "predictor_0      3.510083\n",
      "predictor_10     3.523388\n",
      "dtype: float64\n",
      "const           8.690053\n",
      "predictor_0     3.524311\n",
      "predictor_2     3.342027\n",
      "predictor_10    3.382939\n",
      "dtype: float64\n",
      "const           7.414975\n",
      "predictor_0     3.458816\n",
      "predictor_2     3.317891\n",
      "predictor_3     2.902036\n",
      "predictor_10    3.214744\n",
      "dtype: float64\n",
      "const           6.243559\n",
      "predictor_0     3.464717\n",
      "predictor_2     3.315681\n",
      "predictor_3     2.705245\n",
      "predictor_6     2.587181\n",
      "predictor_10    3.192922\n",
      "dtype: float64\n",
      "[[3.3233]\n",
      " [0.4415]\n",
      " [3.4706]\n",
      " [2.5873]\n",
      " [1.7095]\n",
      " [1.0666]\n",
      " [2.6614]\n",
      " [0.    ]\n",
      " [0.3393]\n",
      " [0.6786]\n",
      " [3.3858]\n",
      " [1.1098]\n",
      " [0.6567]\n",
      " [0.3652]\n",
      " [0.9809]\n",
      " [0.1272]\n",
      " [0.    ]\n",
      " [1.0579]\n",
      " [2.0073]\n",
      " [0.6231]]\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print(models[i].params)\n",
    "print(beta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the data above, we can construct the plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f88b392be20>]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "y_val = [0.196461, 0.231987816, 0.238630727, 0.412314843, 0.317206668]\n",
    "plt.plot(range(1,6), y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the plot is kind of an inverse of the MSE plot as the values increase with increasing model size. This can be explained by the fact that the increased number of predictors will result to higher difference between the actual weights and calculated weights, simply due to their increased number. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11. Ridge regerssion (20pt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1) Perform direct approach to optimize the ridge regression on ````Boston```` dataset, and store the estimated beta to ````beta_direct````.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "boston = pd.read_csv(\"data/Boston.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directly learned parameters: [34.5513 -0.9499]\n"
     ]
    }
   ],
   "source": [
    "from numpy.linalg import inv\n",
    "import statsmodels.api as sm\n",
    "# regression using 2 columns\n",
    "X_train = boston[\"lstat\"].values\n",
    "X_train = sm.add_constant(X_train)\n",
    "y_true = boston['medv'].values\n",
    "ridge_lambda = 0.01\n",
    "\n",
    "# Write your code here\n",
    "beta_direct = inv(X_train.T.dot(X_train)+ridge_lambda).dot(X_train.T).dot(y_true)\n",
    "print(\"Directly learned parameters: {}\".format(beta_direct))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2) Perform gradient descent (Batch gradient descent) to optimize the ridge regression on ````Boston```` dataset, and store the estimated beta to ````beta````.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learned parameters through GD: [32.4884 -0.8791]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# regression using all input columns\n",
    "X_train = boston[[\"lstat\"]].values\n",
    "X_train = sm.add_constant(X_train)\n",
    "y_train = boston['medv'].values\n",
    "\n",
    "beta = np.random.rand(2)\n",
    "alpha = 0.001  # The learning Rate\n",
    "epochs = 20000  # The number of iterations to perform gradient descent\n",
    "n = float(len(X_train)) # Number of elements in X\n",
    "ridge_lambda = 0.01\n",
    "cost = []\n",
    "\n",
    "batch_size = 10\n",
    "num_batches = int(n//batch_size)\n",
    "for epoch in range(epochs):\n",
    "    for batch in range(num_batches):\n",
    "        X_batch = X_train[batch*batch_size:batch*batch_size+batch_size]\n",
    "        y_batch = y_train[batch*batch_size:batch*batch_size+batch_size]\n",
    "        y_pred = np.dot(X_batch, beta)\n",
    "        error = y_pred - y_batch    \n",
    "        ridge_reg_term = (ridge_lambda/2*batch_size) * np.sum(np.square(beta))\n",
    "        dbeta = (1/batch_size) * np.dot(X_batch.T, error) + (ridge_lambda*beta)\n",
    "        #dbeta = (1/batch_size) * np.dot(X_batch.T, error)\n",
    "        beta = beta - alpha * dbeta\n",
    "        #J = (1/2*batch_size) * np.dot(error.T, error)\n",
    "        J = (1/2*batch_size) * np.dot(error.T, error) + ridge_reg_term\n",
    "    cost.append(J)\n",
    "\n",
    "plt.plot(np.arange(epochs), cost)\n",
    "plt.title(\"Learning curve\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(r\"J($\\beta$)\");\n",
    "\n",
    "print(\"Learned parameters through GD: {}\".format(beta))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
